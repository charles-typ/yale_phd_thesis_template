
%% Disaggregation
@inproceedings{mind,
author = {Lee, Seung-seob and Yu, Yanpeng and Tang, Yupeng and Khandelwal, Anurag and Zhong, Lin and Bhattacharjee, Abhishek},
title = {{MIND: In-Network Memory Management for Disaggregated Data Centers}},
year = {2021},
booktitle = {{{SOSP}}},
}

@inproceedings {legoos,
  author = {Yizhou Shan and Yutong Huang and Yilun Chen and Yiying Zhang},
  title = {{LegoOS: A Disseminated, Distributed OS for Hardware Resource Disaggregation}},
  booktitle = {OSDI},
  year = {2018},
}

@inproceedings{disagg,
  title={{Network Requirements for Resource Disaggregation}},
  author={Gao, Peter Xiang and Narayan, Akshay and Karandikar, Sagar and Carreira, Joao and Han, Sangjin and Agarwal, Rachit and Ratnasamy, Sylvia and Shenker, Scott},
  booktitle={OSDI},
  year={2016}
}

@conference {memdisagg1,
  author = {Krste Asanovi{\'c}},
  title = {{FireBox: A Hardware Building Block for 2020 Warehouse-Scale Computers}},
  year = {2014},
}

@inproceedings{memdisagg2,
  author = {Novakovic, Stanko and Daglis, Alexandros and Bugnion, Edouard and Falsafi, Babak and Grot, Boris},
  title = {{Scale-out NUMA}},
  year = {2014},
  booktitle = {ASPLOS},
}

@inproceedings{memdisagg3,
  title={{Memory Disaggregation: Research Problems and Opportunities}},
  author={Liu, Ling and Cao, Wenqi and Sahin, Semih and Zhang, Qi and Bae, Juhyun and Wu, Yanzhao},
  booktitle={ICDCS},
  year={2019},
}

@inproceedings{memdisagg4,
  author = {Lim, Kevin and Chang, Jichuan and Mudge, Trevor and Ranganathan, Parthasarathy and Reinhardt, Steven K. and Wenisch, Thomas F.},
  title = {{Disaggregated Memory for Expansion and Sharing in Blade Servers}},
  year = {2009},
  booktitle = {ISCA},
}

@INPROCEEDINGS{memdisagg5,
  author={K. {Lim} and Y. {Turner} and J. R. {Santos} and A. {AuYoung} and J. {Chang} and P. {Ranganathan} and T. F. {Wenisch}},
  booktitle={HPCA}, 
  title={{System-level Implications of Disaggregated Memory}}, 
  year={2012},
}

@Inbook{memdisagg6,
  author="Samih, Ahmad and Wang, Ren and Maciocco, Christian and Kharbutli, Mazen and Solihin, Yan",
  title="Collaborative Memories in Clusters: Opportunities and Challenges",
  bookTitle="Transactions on Computational Science XXII",
  year="2014",
}

@inproceedings{nwsupport,
author = {Han, Sangjin and Egi, Norbert and Panda, Aurojit and Ratnasamy, Sylvia and Shi, Guangyu and Shenker, Scott},
title = {{Network Support for Resource Disaggregation in Next-Generation Datacenters}},
year = {2013},
booktitle = {HotNets},
}

@inproceedings{pegasus,
	author    = {Jialin Li and Jacob Nelson and Ellis Michael and Xin Jin and Dan R. K. Ports},
	title     = {{Pegasus: Tolerating Skewed Workloads in Distributed Storage with In-Network Coherence Directories}},
	booktitle = {OSDI},
	year      = {2020}
}

@inproceedings {infiniswap,
  author = {Juncheng Gu and Youngmoon Lee and Yiwen Zhang and Mosharaf Chowdhury and Kang G. Shin},
  title = {{Efficient Memory Disaggregation with Infiniswap}},
  booktitle = {NSDI},
  year = {2017},
}

@inproceedings{disaggfault,
  author = {Carbonari, Amanda and Beschasnikh, Ivan},
  title = {{Tolerating Faults in Disaggregated Datacenters}},
  year = {2017},
  booktitle = {HotNets},
}

@inproceedings {snowset,
author = {Midhul Vuppalapati and Justin Miron and Rachit Agarwal and Dan Truong and Ashish Motivala and Thierry Cruanes},
title = {Building an Elastic Query Engine on Disaggregated Storage},
booktitle = {{USENIX} Networked Systems Design and Implementation ({USENIX} {NSDI}'20)},
}

%% Compiler or Runtime

@inproceedings{wang2020_semeru,
  title = {Semeru: {{A Memory-Disaggregated Managed Runtime}}},
  shorttitle = {Semeru},
  booktitle = {OSDI},
  author = {Wang, Chenxi and Ma, Haoran and Liu, Shi and Li, Yuanqi and Ruan, Zhenyuan and Nguyen, Khanh and Bond, Michael D. and Netravali, Ravi and Kim, Miryung and Xu, Guoqing Harry},
  year = {2020},
  pages = {261--280},
}

@inproceedings{wang2022_memLiner,
  title = {{{MemLiner}}: {{Lining}} up Tracing and Application for a {{Far-Memory-Friendly}} Runtime},
  booktitle = {OSDI},
  author = {Wang, Chenxi and Ma, Haoran and Liu, Shi and Qiao, Yifan and Eyolfson, Jonathan and Navasca, Christian and Lu, Shan and Xu, Guoqing Harry},
  year = {2022},
  month = jul,
  pages = {35--53},
}

@inproceedings{serverlessdisaggregation,
  title={The Serverless Data Center : Hardware Disaggregation Meets Serverless Computing},
  author={Nathan Pemberton and Johann Schleier-Smith},
  year={2019},
  url={https://api.semanticscholar.org/CorpusID:115138760}
}


@misc{gcs,
      title={GCS: Generalized Cache Coherence For Efficient Synchronization}, 
      author={Yanpeng Yu and Seung-seob Lee and Anurag Khandelwal and Lin Zhong},
      year={2023},
      eprint={2301.02576},
      archivePrefix={arXiv},
      primaryClass={cs.DC}
}

@inproceedings{memtis,
author = {Lee, Taehyung and Monga, Sumit Kumar and Min, Changwoo and Eom, Young Ik},
title = {MEMTIS: Efficient Memory Tiering with Dynamic Page Classification and Page Size Determination},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613167},
doi = {10.1145/3600006.3613167},
abstract = {The evergrowing memory demand fueled by datacenter workloads is the driving force behind new memory technology innovations (e.g., NVM, CXL). Tiered memory is a promising solution which harnesses such multiple memory types with varying capacity, latency, and cost characteristics in an effort to reduce server hardware costs while fulfilling memory demand. Prior works on memory tiering make suboptimal (often pathological) page placement decisions because they rely on various heuristics and static thresholds without considering overall memory access distribution. Also, deciding the appropriate page size for an application is difficult as huge pages are not always beneficial as a result of skewed accesses within them. We present Memtis, a tiered memory system that adopts informed decision-making for page placement and page size determination. Memtis leverages access distribution of allocated pages to optimally approximate the hot data set to the fast tier capacity. Moreover, Memtis dynamically determines the page size that allows applications to use huge pages while avoiding their drawbacks by detecting inefficient use of fast tier memory and splintering them if necessary. Our evaluation shows that Memtis outperforms state-of-the-art tiering systems by up to 169.0\% and their best by up to 33.6\%.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {17–34},
numpages = {18},
keywords = {tiered memory management, virtual memory, operating system},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@inproceedings{memory1,
author = {Rao, Pramod Subba and Porter, George},
title = {Is Memory Disaggregation Feasible? A Case Study with Spark SQL},
year = {2016},
isbn = {9781450341837},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2881025.2881030},
doi = {10.1145/2881025.2881030},
abstract = {This paper explores the feasibility of entirely disaggregated memory from compute and storage for a particular, widely deployed workload, Spark SQL analytics queries. We measure the empirical rate at which records are processed and calculate the effective memory bandwidth utilized based on the sizes of the columns accessed in the query. Our findings contradict conventional wisdom: not only is memory disaggregation possible under this workload, but achievable with already available, commercial network technology. Beyond this finding, we also recommend changes that can be made to Spark SQL to improve its ability to support memory disaggregation.},
booktitle = {Proceedings of the 2016 Symposium on Architectures for Networking and Communications Systems},
pages = {75–80},
numpages = {6},
location = {Santa Clara, California, USA},
series = {ANCS '16}
}

@inproceedings{memory2,
author = {Verma, Abhishek and Pedrosa, Luis and Korupolu, Madhukar and Oppenheimer, David and Tune, Eric and Wilkes, John},
title = {Large-scale cluster management at Google with Borg},
year = {2015},
isbn = {9781450332385},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2741948.2741964},
doi = {10.1145/2741948.2741964},
abstract = {Google's Borg system is a cluster manager that runs hundreds of thousands of jobs, from many thousands of different applications, across a number of clusters each with up to tens of thousands of machines.It achieves high utilization by combining admission control, efficient task-packing, over-commitment, and machine sharing with process-level performance isolation. It supports high-availability applications with runtime features that minimize fault-recovery time, and scheduling policies that reduce the probability of correlated failures. Borg simplifies life for its users by offering a declarative job specification language, name service integration, real-time job monitoring, and tools to analyze and simulate system behavior.We present a summary of the Borg system architecture and features, important design decisions, a quantitative analysis of some of its policy decisions, and a qualitative examination of lessons learned from a decade of operational experience with it.},
booktitle = {Proceedings of the Tenth European Conference on Computer Systems},
articleno = {18},
numpages = {17},
location = {Bordeaux, France},
series = {EuroSys '15}
}

@inproceedings{memory3,
author = {Reiss, Charles and Tumanov, Alexey and Ganger, Gregory R. and Katz, Randy H. and Kozuch, Michael A.},
title = {Heterogeneity and dynamicity of clouds at scale: Google trace analysis},
year = {2012},
isbn = {9781450317610},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2391229.2391236},
doi = {10.1145/2391229.2391236},
abstract = {To better understand the challenges in developing effective cloud-based resource schedulers, we analyze the first publicly available trace data from a sizable multi-purpose cluster. The most notable workload characteristic is heterogeneity: in resource types (e.g., cores:RAM per machine) and their usage (e.g., duration and resources needed). Such heterogeneity reduces the effectiveness of traditional slot- and core-based scheduling. Furthermore, some tasks are constrained as to the kind of machine types they can use, increasing the complexity of resource assignment and complicating task migration. The workload is also highly dynamic, varying over time and most workload features, and is driven by many short jobs that demand quick scheduling decisions. While few simplifying assumptions apply, we find that many longer-running jobs have relatively stable resource utilizations, which can help adaptive resource schedulers.},
booktitle = {Proceedings of the Third ACM Symposium on Cloud Computing},
articleno = {7},
numpages = {13},
location = {San Jose, California},
series = {SoCC '12}
}

@inproceedings{memory4,
author = {Gunawi, Haryadi S. and Hao, Mingzhe and Suminto, Riza O. and Laksono, Agung and Satria, Anang D. and Adityatama, Jeffry and Eliazar, Kurnia J.},
title = {Why Does the Cloud Stop Computing? Lessons from Hundreds of Service Outages},
year = {2016},
isbn = {9781450345255},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2987550.2987583},
doi = {10.1145/2987550.2987583},
abstract = {We conducted a cloud outage study (COS) of 32 popular Internet services. We analyzed 1247 headline news and public post-mortem reports that detail 597 unplanned outages that occurred within a 7-year span from 2009 to 2015. We analyzed outage duration, root causes, impacts, and fix procedures. This study reveals the broader availability landscape of modern cloud services and provides answers to why outages still take place even with pervasive redundancies.},
booktitle = {Proceedings of the Seventh ACM Symposium on Cloud Computing},
pages = {1–16},
numpages = {16},
location = {Santa Clara, CA, USA},
series = {SoCC '16}
}

@inproceedings{memory5,
author = {Benson, Theophilus and Akella, Aditya and Maltz, David A.},
title = {Network traffic characteristics of data centers in the wild},
year = {2010},
isbn = {9781450304832},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1879141.1879175},
doi = {10.1145/1879141.1879175},
abstract = {Although there is tremendous interest in designing improved networks for data centers, very little is known about the network-level traffic characteristics of data centers today. In this paper, we conduct an empirical study of the network traffic in 10 data centers belonging to three different categories, including university, enterprise campus, and cloud data centers. Our definition of cloud data centers includes not only data centers employed by large online service providers offering Internet-facing applications but also data centers used to host data-intensive (MapReduce style) applications). We collect and analyze SNMP statistics, topology and packet-level traces. We examine the range of applications deployed in these data centers and their placement, the flow-level and packet-level transmission properties of these applications, and their impact on network and link utilizations, congestion and packet drops. We describe the implications of the observed traffic patterns for data center internal traffic engineering as well as for recently proposed architectures for data center networks.},
booktitle = {Proceedings of the 10th ACM SIGCOMM Conference on Internet Measurement},
pages = {267–280},
numpages = {14},
keywords = {data center traffic, characterization},
location = {Melbourne, Australia},
series = {IMC '10}
}

@INPROCEEDINGS{memory6,
  author={Hines, Michael R. and Gordon, Abel and Silva, Marcio and Da Silva, Dilma and Ryu, Kyung and Ben-Yehuda, Muli},
  booktitle={2011 IEEE Third International Conference on Cloud Computing Technology and Science}, 
  title={Applications Know Best: Performance-Driven Memory Overcommit with Ginkgo}, 
  year={2011},
  volume={},
  number={},
  pages={130-137},
  keywords={Memory management;Virtual machine monitors;Load modeling;Virtual machining;Java;Servers;Correlators;Cloud Computing;Virtualization;Over-subscription;Overcommittment},
  doi={10.1109/CloudCom.2011.27}}

@inproceedings {memory7,
author = {Diwaker Gupta and Sangmin Lee and Michael Vrable and Stefan Savage and Alex C. Snoeren and George Varghese and Geoffrey M. Voelker and Amin Vahdat},
title = {Difference Engine: Harnessing Memory Redundancy in Virtual Machines},
booktitle = {8th USENIX Symposium on Operating Systems Design and Implementation (OSDI 08)},
year = {2008},
address = {San Diego, CA},
url = {https://www.usenix.org/conference/osdi-08/difference-engine-harnessing-memory-redundancy-virtual-machines},
publisher = {USENIX Association},
month = dec
}

@article{memory8,
author = {Bod\'{\i}k, Peter and Menache, Ishai and Chowdhury, Mosharaf and Mani, Pradeepkumar and Maltz, David A. and Stoica, Ion},
title = {Surviving failures in bandwidth-constrained datacenters},
year = {2012},
issue_date = {October 2012},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {42},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2377677.2377760},
doi = {10.1145/2377677.2377760},
abstract = {Datacenter networks have been designed to tolerate failures of network equipment and provide sufficient bandwidth. In practice, however, failures and maintenance of networking and power equipment often make tens to thousands of servers unavailable, and network congestion can increase service latency. Unfortunately, there exists an inherent tradeoff between achieving high fault tolerance and reducing bandwidth usage in network core; spreading servers across fault domains improves fault tolerance, but requires additional bandwidth, while deploying servers together reduces bandwidth usage, but also decreases fault tolerance. We present a detailed analysis of a large-scale Web application and its communication patterns. Based on that, we propose and evaluate a novel optimization framework that achieves both high fault tolerance and significantly reduces bandwidth usage in the network core by exploiting the skewness in the observed communication patterns.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {431–442},
numpages = {12},
keywords = {bandwidth, datacenter networks, fault tolerance}
}

@inproceedings {memory9,
author = {Ali Ghodsi and Matei Zaharia and Benjamin Hindman and Andy Konwinski and Scott Shenker and Ion Stoica},
title = {Dominant Resource Fairness: Fair Allocation of Multiple Resource Types},
booktitle = {8th USENIX Symposium on Networked Systems Design and Implementation (NSDI 11)},
year = {2011},
address = {Boston, MA},
url = {https://www.usenix.org/conference/nsdi11/dominant-resource-fairness-fair-allocation-multiple-resource-types},
publisher = {USENIX Association},
month = mar
}

@article{memory10,
author = {Grandl, Robert and Ananthanarayanan, Ganesh and Kandula, Srikanth and Rao, Sriram and Akella, Aditya},
title = {Multi-resource packing for cluster schedulers},
year = {2014},
issue_date = {October 2014},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {44},
number = {4},
issn = {0146-4833},
url = {https://doi.org/10.1145/2740070.2626334},
doi = {10.1145/2740070.2626334},
abstract = {Tasks in modern data parallel clusters have highly diverse resource requirements, along CPU, memory, disk and network. Any of these resources may become bottlenecks and hence, the likelihood of wasting resources due to fragmentation is now larger. Today's schedulers do not explicitly reduce fragmentation. Worse, since they only allocate cores and memory, the resources that they ignore (disk and network) can be over-allocated leading to interference, failures and hogging of cores or memory that could have been used by other tasks. We present Tetris, a cluster scheduler that packs, i.e., matches multi-resource task requirements with resource availabilities of machines so as to increase cluster efficiency (makespan). Further, Tetris uses an analog of shortest-running-time-first to trade-off cluster efficiency for speeding up individual jobs. Tetris' packing heuristics seamlessly work alongside a large class of fairness policies. Trace-driven simulations and deployment of our prototype on a 250 node cluster shows median gains of 30\% in job completion time while achieving nearly perfect fairness.},
journal = {SIGCOMM Comput. Commun. Rev.},
month = {aug},
pages = {455–466},
numpages = {12},
keywords = {packing, multi-dimensional, makespan, fairness, completion time, cluster schedulers}
}

@inproceedings{pool1,
  title={Logical Memory Pools: Flexible and Local Disaggregated Memory},
  author={Amaro, Emmanuel and Wang, Stephanie and Panda, Aurojit and Aguilera, Marcos K},
  booktitle={Proceedings of the 22nd ACM Workshop on Hot Topics in Networks},
  pages={25--32},
  year={2023}
}

@article{pool2,
  title={MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool},
  author={Hu, Cunchen and Huang, Heyang and Hu, Junhao and Xu, Jiang and Chen, Xusheng and Xie, Tao and Wang, Chenxi and Wang, Sa and Bao, Yungang and Sun, Ninghui and others},
  journal={arXiv preprint arXiv:2406.17565},
  year={2024}
}

@inproceedings{existing1,
  title={One-sided $\{$RDMA-Conscious$\}$ extendible hashing for disaggregated memory},
  author={Zuo, Pengfei and Sun, Jiazhao and Yang, Liu and Zhang, Shuangwu and Hua, Yu},
  booktitle={2021 USENIX Annual Technical Conference (USENIX ATC 21)},
  pages={15--29},
  year={2021}
}

@inproceedings{runtime1,
  title={Rethinking software runtimes for disaggregated memory},
  author={Calciu, Irina and Imran, M Talha and Puddu, Ivan and Kashyap, Sanidhya and Maruf, Hasan Al and Mutlu, Onur and Kolli, Aasheesh},
  booktitle={Proceedings of the 26th ACM International Conference on Architectural Support for Programming Languages and Operating Systems},
  pages={79--92},
  year={2021}
}

@inproceedings{runtime2,
  title={Semeru: A $\{$Memory-Disaggregated$\}$ managed runtime},
  author={Wang, Chenxi and Ma, Haoran and Liu, Shi and Li, Yuanqi and Ruan, Zhenyuan and Nguyen, Khanh and Bond, Michael D and Netravali, Ravi and Kim, Miryung and Xu, Guoqing Harry},
  booktitle={14th USENIX Symposium on Operating Systems Design and Implementation (OSDI 20)},
  pages={261--280},
  year={2020}
}