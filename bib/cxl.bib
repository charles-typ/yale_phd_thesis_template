

@misc{cxl,
  title        = {{Compute Express Link (CXL)}},
  howpublished = {\url{https://www.computeexpresslink.org/}}
}

@article{cxl_azure,
  title={First-generation Memory Disaggregation for Cloud Platforms},
  author={Li, Huaicheng and Berger, Daniel S and Novakovic, Stanko and Hsu, Lisa and Ernst, Dan and Zardoshti, Pantea and Shah, Monish and Agarwal, Ishwar and Hill, Mark and Fontoura, Marcus and others},
  journal={arXiv preprint arXiv:2203.00241},
  year={2022}
}

@misc{cxlcentric,
      title={A Case for CXL-Centric Server Processors}, 
      author={Albert Cho and Anish Saxena and Moinuddin Qureshi and Alexandros Daglis},
      year={2023},
      eprint={2305.05033},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}


@article{pond,
  title={Pond: CXL-Based Memory Pooling Systems for Cloud Platforms},
  author={Huaicheng Li and Daniel S. Berger and Stanko Novakovic and Lisa R. Hsu and Dan Ernst and Pantea Zardoshti and Monish Shah and Samir Rajadnya and Scott Lee and Ishwar Agarwal and Mark D. Hill and Marcus Fontoura and Ricardo Bianchini},
  journal={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252907213}
}

@inproceedings{tpp,
author = {Maruf, Hasan Al and Wang, Hao and Dhanotia, Abhishek and Weiner, Johannes and Agarwal, Niket and Bhattacharya, Pallab and Petersen, Chris and Chowdhury, Mosharaf and Kanaujia, Shobhit and Chauhan, Prakash},
title = {TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582063},
doi = {10.1145/3582016.3582063},
abstract = {The increasing demand for memory in hyperscale applications has led to memory becoming a large portion of the overall datacenter spend. The emergence of coherent interfaces like CXL enables main memory expansion and offers an efficient solution to this problem. In such systems, the main memory can constitute different memory technologies with varied characteristics. In this paper, we characterize memory usage patterns of a wide range of datacenter applications across the server fleet of Meta. We, therefore, demonstrate the opportunities to offload colder pages to slower memory tiers for these applications. Without efficient memory management, however, such systems can significantly degrade performance. We propose a novel OS-level application-transparent page placement mechanism (TPP) for CXL-enabled memory. TPP employs a lightweight mechanism to identify and place hot/cold pages to appropriate memory tiers. It enables a proactive page demotion from local memory to CXL-Memory. This technique ensures a memory headroom for new page allocations that are often related to request processing and tend to be short-lived and hot. At the same time, TPP can promptly promote performance-critical hot pages trapped in the slow CXL-Memory to the fast local memory, while minimizing both sampling overhead and unnecessary migrations. TPP works transparently without any application-specific knowledge and can be deployed globally as a kernel release. We evaluate TPP with diverse memory-sensitive workloads in the production server fleet with early samples of new x86 CPUs with CXL 1.1 support. TPP makes a tiered memory system performant as an ideal baseline (&lt;1\% gap) that has all the memory in the local tier. It is 18\% better than today’s Linux, and 5–17\% better than existing solutions including NUMA Balancing and AutoTiering. Most of the TPP patches have been merged in the Linux v5.18 release while the remaining ones are just pending for more discussion.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {742–755},
numpages = {14},
keywords = {Operating Systems, Memory Management, Datacenters, Tiered-Memory, Heterogeneous System, CXL-Memory},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings {mt2,
author = {Jifei Yi and Benchao Dong and Mingkai Dong and Ruizhe Tong and Haibo Chen},
title = {{MT\^2}: Memory Bandwidth Regulation on Hybrid {NVM/DRAM} Platforms},
booktitle = {20th USENIX Conference on File and Storage Technologies (FAST 22)},
year = {2022},
isbn = {978-1-939133-26-7},
address = {Santa Clara, CA},
pages = {199--216},
url = {https://www.usenix.org/conference/fast22/presentation/yi-mt2},
publisher = {USENIX Association},
month = feb,
}

@inproceedings {nyxcache,
author = {Kan Wu and Kaiwei Tu and Yuvraj Patel and Rathijit Sen and Kwanghyun Park and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},
title = {{NyxCache}: Flexible and Efficient Multi-tenant Persistent Memory Caching},
booktitle = {20th USENIX Conference on File and Storage Technologies (FAST 22)},
year = {2022},
isbn = {978-1-939133-26-7},
address = {Santa Clara, CA},
pages = {1--16},
url = {https://www.usenix.org/conference/fast22/presentation/wu},
publisher = {USENIX Association},
month = feb,
}

@inproceedings{numalloc,
author = {Yang, Hanmei and Zhao, Xin and Zhou, Jin and Wang, Wei and Kundu, Sandip and Wu, Bo and Guan, Hui and Liu, Tongping},
title = {NUMAlloc: A Faster NUMA Memory Allocator},
year = {2023},
isbn = {9798400701795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591195.3595276},
doi = {10.1145/3591195.3595276},
abstract = {The NUMA architecture accommodates the hardware trend of an increasing number of CPU cores. It requires the cooperation of memory allocators to achieve good performance for multithreaded applications. Unfortunately, existing allocators do not support NUMA architecture well. This paper presents a novel memory allocator – NUMAlloc, that is designed for the NUMA architecture. is centered on a binding-based memory management. On top of it, proposes an “origin-aware memory management” to ensure the locality of memory allocations and deallocations, as well as a method called “incremental sharing” to balance the performance benefits and memory overhead of using transparent huge pages. According to our extensive evaluation, NUMAlloc has the best performance among all evaluated allocators, running 15.7\% faster than the second-best allocator (mimalloc), and 20.9\% faster than the default Linux allocator with reasonable memory overhead. NUMAlloc is also scalable to 128 threads and is ready for deployment.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on Memory Management},
pages = {97–110},
numpages = {14},
keywords = {NUMA Architecture, Memory Allocation},
location = {Orlando, FL, USA},
series = {ISMM 2023}
}

@ARTICLE{smt,
  author={Kim, Kyungsan and Kim, Hyunseok and So, Jinin and Lee, Wonjae and Im, Junhyuk and Park, Sungjoo and Cho, Jeonghyeon and Song, Hoyoung},
  journal={IEEE Micro}, 
  title={SMT: Software-Defined Memory Tiering for Heterogeneous Computing Systems With CXL Memory Expander}, 
  year={2023},
  volume={43},
  number={2},
  pages={20-29},
  doi={10.1109/MM.2023.3240774}}

@inproceedings {empiricalPM,
author = {Jian Yang and Juno Kim and Morteza Hoseinzadeh and Joseph Izraelevitz and Steve Swanson},
title = {An Empirical Guide to the Behavior and Use of Scalable Persistent Memory},
booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
year = {2020},
isbn = {978-1-939133-12-0},
address = {Santa Clara, CA},
pages = {169--182},
url = {https://www.usenix.org/conference/fast20/presentation/yang},
publisher = {USENIX Association},
month = feb,
}

@ARTICLE{cxltradeoff,
  author={Berger, Daniel S. and Ernst, Daniel and Li, Huaicheng and Zardoshti, Pantea and Shah, Monish and Rajadnya, Samir and Lee, Scott and Hsu, Lisa and Agarwal, Ishwar and Hill, Mark D. and Bianchini, Ricardo},
  journal={IEEE Micro}, 
  title={Design Tradeoffs in CXL-Based Memory Pools for Public Cloud Platforms}, 
  year={2023},
  volume={43},
  number={2},
  pages={30-38},
  doi={10.1109/MM.2023.3241586}}

@misc{demystify,
      title={Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices}, 
      author={Yan Sun and Yifan Yuan and Zeduo Yu and Reese Kuper and Ipoom Jeong and Ren Wang and Nam Sung Kim},
      year={2023},
      eprint={2303.15375},
      archivePrefix={arXiv},
      primaryClass={cs.PF}
}

@misc{Interleavepatch,
  title = {{J. Weiner. [PATCH] mm: mempolicy: N:M interleave policy for tiered memory nodes.}},
  howpublished = "\url{https://lore.kernel.org/linux-mm/YqD0%2FtzFwXvJ1gK6@cmpxchg.org/T/}",
}

@misc{mlc,
  title = {{V. Viswanathan, K. Kumar, and T. Willhalm. "Intel® Memory Latency Checker v3.10"}},
  howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html}",
}

@inproceedings {directcxl,
author = {Donghyun Gouk and Sangwon Lee and Miryeong Kwon and Myoungsoo Jung},
title = {Direct Access, {High-Performance} Memory Disaggregation with {DirectCXL}},
booktitle = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
year = {2022},
isbn = {978-1-939133-29-65},
address = {Carlsbad, CA},
pages = {287--294},
url = {https://www.usenix.org/conference/atc22/presentation/gouk},
publisher = {USENIX Association},
month = jul,
}


@misc{A1000,
  title = {{Leo Memory Connectivity Platform for CXL 1.1 and 2.0}},
  howpublished = "\url{https://www.asteralabs.com/wp-content/uploads/2022/08/Astera_Labs_Leo_Aurora_Product_FINAL.pdf}",
}

@misc{SPR,
  title = {{Intel Corporation. Intel launches $\text{4}^{th}$ gen xeon scalable processors, max series cpus.}},
  howpublished = "\url{https://www.intel.com/content/www/us/en/newsroom/news/}",
}

@misc(EPYC,
  title = {{AMD. AMD EPYC 9004 Series Server Processors}},
  howpublished = "\url{https://www.amd.com/en/processors/epyc-9004-series}",
)

@misc{mxc,
  title = {{Montage Technology. Cxl memory expander controller (mxc).}},
  howpublished = "\url{https://www.montage-tech.com/MXC, accessed in 2023.}",
}

@misc{snc,
    title = {{David L Mulnix. Intel® Xeon® Processor Scalable Family Technical Overview}},
    howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html}",
}

@misc{intelfpga,
  title = {{Intel Corporation. Intel Agilex® 7 FPGA and SoC FPGA I-Series}},
  howpublished = "\url{https://www.intel.com/content/www/us/en/products/details/fpga/agilex/7/i-series.html}",
}

@misc{q8chat,
  title = {{Julien Simon.Smaller is Better: Q8-Chat LLM is an Efficient Generative AI Experience on Intel® Xeon® Processors}},
  howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html}",
}

@misc{emerald_rapids,
    title = {{Intel Corporation. Intel Unveils Future-Generation Xeon with Robust Performance and Efficiency Architectures}},
    howpublished = "\url{https://www.intel.com/content/www/us/en/newsroom/news/intel-unveils-future-generation-xeon.html}",
}

@misc{pcm,
    title = {{Intel Corporation. Intel® Performance Counter Monitor (Intel® PCM)}},
    howpublished = "\url{https://github.com/intel/pcm}",
}

@inproceedings{FlatFlash,
author = {Abulila, Ahmed and Mailthody, Vikram Sharma and Qureshi, Zaid and Huang, Jian and Kim, Nam Sung and Xiong, Jinjun and Hwu, Wen-mei},
title = {FlatFlash: Exploiting the Byte-Accessibility of SSDs within a Unified Memory-Storage Hierarchy},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304061},
doi = {10.1145/3297858.3304061},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {971–985},
numpages = {15},
keywords = {byte-addressable ssd, unified memory management, page promotion, data persistence},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings {cxl-ssd,
author = {Shao-Peng Yang and Minjae Kim and Sanghyun Nam and Juhyung Park and Jin-yong Choi and Eyee Hyun Nam and Eunji Lee and Sungjin Lee and Bryan S. Kim},
title = {Overcoming the Memory Wall with {CXL-Enabled} {SSDs}},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {601--617},
url = {https://www.usenix.org/conference/atc23/presentation/yang-shao-peng},
publisher = {USENIX Association},
month = jul
}

@INPROCEEDINGS{PSACS,
  author={Zou, Chen and Zhang, Hui and Chien, Andrew A. and Seok Ki, Yang},
  booktitle={2021 IEEE 39th International Conference on Computer Design (ICCD)}, 
  title={PSACS: Highly-Parallel Shuffle Accelerator on Computational Storage}, 
  year={2021},
  volume={},
  number={},
  pages={480-487},
  doi={10.1109/ICCD53106.2021.00080}}

@misc{redis,
    title = {{Redis. }},
    howpublished = "\url{https://redis.io/}",
}

@misc{googlecloud,
    title = {{Google Cloud. Memory management best practices}},
    howpublished = "\url{https://cloud.google.com/memorystore/docs/redis/memory-management-best-practices}",
}

@misc{manageredis,
    title = {{Tecton.ai. Managing your Redis Cluster}},
    howpublished = "\url{https://docs.tecton.ai/docs/0.5/setting-up-tecton/setting-up-other-components/managing-your-redis-cluster}",
}

@misc{H100,
    title = {{NVIDIA. NVIDIA H100 Tensor Core GPU}},
    howpublished = "\url{https://www.nvidia.com/en-gb/data-center/h100/}"},
}

@INPROCEEDINGS{amx,
  author={Nassif, Nevine and Munch, Ashley O. and Molnar, Carleton L. and Pasdast, Gerald and Lyer, Sitaraman V. and Yang, Zibing and Mendoza, Oscar and Huddart, Mark and Venkataraman, Srikrishnan and Kandula, Sireesha and Marom, Rafi and Kern, Alexandra M. and Bowhill, Bill and Mulvihill, David R. and Nimmagadda, Srikanth and Kalidindi, Varma and Krause, Jonathan and Haq, Mohammad M. and Sharma, Roopali and Duda, Kevin},
  booktitle={2022 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  title={Sapphire Rapids: The Next-Generation Intel Xeon Scalable Processor}, 
  year={2022},
  volume={65},
  number={},
  pages={44-46},
  doi={10.1109/ISSCC42614.2022.9731107}}

@inproceedings{YCSB,
author = {Cooper, Brian F. and Silberstein, Adam and Tam, Erwin and Ramakrishnan, Raghu and Sears, Russell},
title = {Benchmarking Cloud Serving Systems with YCSB},
year = {2010},
isbn = {9781450300360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807128.1807152},
doi = {10.1145/1807128.1807152},
abstract = {While the use of MapReduce systems (such as Hadoop) for large scale data analysis has been widely recognized and studied, we have recently seen an explosion in the number of systems developed for cloud data serving. These newer systems address "cloud OLTP" applications, though they typically do not support ACID transactions. Examples of systems proposed for cloud serving use include BigTable, PNUTS, Cassandra, HBase, Azure, CouchDB, SimpleDB, Voldemort, and many others. Further, they are being applied to a diverse range of applications that differ considerably from traditional (e.g., TPC-C like) serving workloads. The number of emerging cloud serving systems and the wide range of proposed applications, coupled with a lack of apples-to-apples performance comparisons, makes it difficult to understand the tradeoffs between systems and the workloads for which they are suited. We present the "Yahoo! Cloud Serving Benchmark" (YCSB) framework, with the goal of facilitating performance comparisons of the new generation of cloud data serving systems. We define a core set of benchmarks and report results for four widely used systems: Cassandra, HBase, Yahoo!'s PNUTS, and a simple sharded MySQL implementation. We also hope to foster the development of additional cloud benchmark suites that represent other classes of applications by making our benchmark tool available via open source. In this regard, a key feature of the YCSB framework/tool is that it is extensible--it supports easy definition of new workloads, in addition to making it easy to benchmark new systems.},
booktitle = {Proceedings of the 1st ACM Symposium on Cloud Computing},
pages = {143–154},
numpages = {12},
keywords = {benchmarking, cloud serving database},
location = {Indianapolis, Indiana, USA},
series = {SoCC '10}
}

@misc{cpuinference,
    title = {{Timothy P. Morgan. WHY AI INFERENCE WILL REMAIN LARGELY ON THE CPU}},
    howpublished = "\url{https://www.nextplatform.com/2023/04/05/why-ai-inference-will-remain-largely-on-the-cpu/}",
}

@misc{intelbenchmark,
    title = {{Intel Corporation. 4th Generation Intel® Xeon® Scalable Processors Performance.}},
    howpublished = "\url{https://edc.intel.com/content/www/us/en/products/performance/benchmarks/4th-generation-intel-xeon-scalable-processors/}",
}

@misc{vllm,
      title={Efficient Memory Management for Large Language Model Serving with PagedAttention}, 
      author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
      year={2023},
      eprint={2309.06180},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lightllm,
    title={{Lightllm A Light and Fast Inference Service for LLM}},
    howpublished = "\url{https://github.com/ModelTC/lightllm}",
}

@misc{amdepyc,
    title={{Videocardz. AMD next-gen socket SP7 reportedly launches alongside Venice data-center CPUs}},
    howpublished = "\url{https://videocardz.com/newz/amd-epyc-venice-with-zen6-cpu-cores-reportedly-uses-new-sp7-socket}",
}

@misc{redis-benchmark,
    title={{Redis benchmark. Using the redis-benchmark utility on a Redis server.}},
    howpublished = "\url{https://lore.kernel.org/lkml/CAAPL-u9Wv+nH1VOZTj=9p9S70Y3Qz3+63EkqncRDdHfubsrjfw@mail.gmail.com/}",
}

@inproceedings{fpgaasic,
author = {Kuon, Ian and Rose, Jonathan},
title = {Measuring the Gap between FPGAs and ASICs},
year = {2006},
isbn = {1595932925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1117201.1117205},
doi = {10.1145/1117201.1117205},
abstract = {This paper presents experimental measurements of the differences between a 90nm CMOS FPGA and 90nm CMOS Standard Cell ASICs in terms of logic density, circuit speed and power consumption. We are motivated to make these measurements to enable system designers to make better informed hoices between these two media and to give insight to FPGA makers on the deficiencies to attack and thereby improve FPGAs. In the paper, we describe the methodology by which the measurements were obtained and we show that, for circuits containing only combinational logic and flip-flops, the ratio of silicon area required to implement them in FPGAs and ASICs is on average 40. Modern FPGAs also contain "hard" blocks such as multiplier/accumulators and block memories and we find that these blocks reduce this average area gap significantly to as little as 21. The ratio of critical path delay, from FPGA to ASIC, is roughly 3 to 4, with less influence from block memory and hard multipliers. The dynamic power onsumption ratio is approximately 12 times and, with hard blocks, this gap generally becomes smaller.},
booktitle = {Proceedings of the 2006 ACM/SIGDA 14th International Symposium on Field Programmable Gate Arrays},
pages = {21–30},
numpages = {10},
keywords = {FPGA, ASIC, area comparison, power comparison, delay comparison},
location = {Monterey, California, USA},
series = {FPGA '06}
}

@misc{intelsnc,
    title={{Intel Xeon Processor Scalable Family Technical Overview.}},
    howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html}",
}

@misc{gh200,
title={{Nvidia GH200 Datasheet.}},
howpublished="\url{https://resources.nvidia.com/en-us-dgx-gh200/nvidia-grace-hopper-superchip-datasheet}",
}
@misc{appleuma,
title={{Apple Introduces M2 Ultra.}},
howpublished="\url{https://www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/}",
}
@misc{pcieopt,
title={{PCI-SIG explores an optical interconnect for higher PCIe performance}},
howpublished="\url{https://www.eenewseurope.com/en/pci-sig-explores-an-optical-connections-for-higher-pcie-performance/}",
}
@misc{uec,
title={{Ultra Ethernet Consortium.}},
howpublished="\url{https://ultraethernet.org/}",
}

@inproceedings{tpuv4,
author = {N. Jouppi and et al.},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
}

@misc{cxl-centric,
title={{A Case for CXL-Centric Server Processors.}},
author={A. Cho and et al.},
howpublished="\url{https://arxiv.org/abs/2305.05033}",
}

@inproceedings{moe-iclr17,
author = {N. Shazeer and et al.},
title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
year = {2017},
booktitle = {The 5th International Conference on Learning Representations},
}

@misc{gpt4,
title={{GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE.}},
author={D. Patel, G. Wong},
year={2023},
howpublished="\url{https://www.semianalysis.com/p/gpt-4-architecture-infrastructure}",
}

@inproceedings{cxldatabase,
author = {Ahn, Minseon and Chang, Andrew and Lee, Donghun and Gim, Jongmin and Kim, Jungmin and Jung, Jaemin and Rebholz, Oliver and Pham, Vincent and Malladi, Krishna and Ki, Yang Seok},
title = {Enabling CXL Memory Expansion for In-Memory Database Management Systems},
year = {2022},
isbn = {9781450393782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533737.3535090},
doi = {10.1145/3533737.3535090},
abstract = {Limited memory volume is always a performance bottleneck in an in-memory database management system (IMDBMS) as the data size keeps increasing. To overcome the physical memory limitation, heterogeneous and disaggregated computing platforms are proposed, such as Gen-Z, CCIX, OpenCAPI, and CXL. In this work, we introduce flexible CXL memory expansion using a CXL type 3 prototype and evaluate its performance in an IMDBMS. Our evaluation shows that CXL memory devices interfaced with PCIe Gen5 are appropriate for memory expansion with nearly no throughput degradation in OLTP workloads and less than 8\% throughput degradation in OLAP workloads. Thus, CXL memory is a good candidate for memory expansion with lower TCO in IMDBMSs.},
booktitle = {Proceedings of the 18th International Workshop on Data Management on New Hardware},
articleno = {8},
numpages = {5},
keywords = {CXL, Database Management Systems, Compute Express Link, In-Memory Database, DBMS},
location = {Philadelphia, PA, USA},
series = {DaMoN '22}
}

@INPROCEEDINGS{snoopfilter,
  author={Sharma, Debendra Das},
  booktitle={2022 IEEE Symposium on High-Performance Interconnects (HOTI)}, 
  title={Compute Express Link®: An open industry-standard interconnect enabling heterogeneous data-centric computing}, 
  year={2022},
  volume={},
  number={},
  pages={5-12},
  doi={10.1109/HOTI55740.2022.00017}}

@article{sparkmemory,
author = {Zhang, Xuechen and Khanal, Ujjwal and Zhao, Xinghui and Ficklin, Stephen},
title = {Making Sense of Performance in In-Memory Computing Frameworks for Scientific Data Analysis: A Case Study of the Spark System},
year = {2018},
issue_date = {Oct 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2017.10.016},
doi = {10.1016/j.jpdc.2017.10.016},
journal = {J. Parallel Distrib. Comput.},
month = {oct},
pages = {369–382},
numpages = {14},
keywords = {SciDB, Spark, In-memory computing, Scientific data analytics}
}

@INPROCEEDINGS{hybridcxleval,
  author={Yang, Qirui and Jin, Runyu and Davis, Bridget and Inupakutika, Devasena and Zhao, Ming},
  booktitle={2022 IEEE International Conference on Networking, Architecture and Storage (NAS)}, 
  title={Performance Evaluation on CXL-enabled Hybrid Memory Pool}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/NAS55553.2022.9925356}}

@misc{h100amazon,
title={{Nvidia H100 80GB on Amazon.}},
year={2023},
howpublished="\url{https://www.amazon.com/Tesla-NVIDIA-Learning-Compute-Graphics/dp/B0C3XH4QSJ}",
}

@misc{msssd,
title={{Samsung Memory Semantic SSD.}},
year={2023},
howpublished="\url{https://samsungmsl.com/ms-ssd/}",
}

@INPROCEEDINGS{elasticcomputing,
  author={Yi, Sangho and Kondo, Derrick and Andrzejak, Artur},
  booktitle={2010 IEEE 3rd International Conference on Cloud Computing}, 
  title={Reducing Costs of Spot Instances via Checkpointing in the Amazon Elastic Compute Cloud}, 
  year={2010},
  volume={},
  number={},
  pages={236-243},
  doi={10.1109/CLOUD.2010.35}}

@misc{awsm7a,
title={{Amazon EC2 M7a Instances}},
year={2023},
howpublished="\url{https://aws.amazon.com/ec2/instance-types/m7a/}",
}

@misc{awsm7i,
title={{Amazon EC2 M7i Instances}},
year={2023},
howpublished="\url{https://aws.amazon.com/ec2/instance-types/m7i/}",
}

@misc{redisenterprise,
title={{Redis enterprise}},
year={2023},
howpublished="\url{https://redis.io/docs/about/redis-enterprise/}",
}

@misc{numaautobalancing,
title={{NUMA balancing: optimize memory placement for memory tiering system}},
howpublished="\url{https://lore.kernel.org/linux-mm/20220221084529.1052339-1-ying.huang@intel.com/}",
}

@misc{tpppatch,
title={{Transparent Page Placement for Tiered-Memory}},
howpublished = "\url{https://lore.kernel.org/all/cover.1637778851.git.hasanalmaruf@fb.com/}",
}

@misc{hot,
title={{Tiered-Memory: Hot page selection}},
howpublished = "\url{https://lore.kernel.org/lkml/20220622083519.708236-2-ying.huang@intel.com/T/}",
}



@misc{redisautotiering,
title={{Auto Tiering Extend Redis Enterprise databases beyond DRAM limits}},
howpublished = "\url{https://redis.com/redis-enterprise/technology/auto-tiering/#:~:text=Redis%20Enterprise's%20auto%20tiering%20lets,compared%20to%20only%20DRAM%20deployments.}",
}


@inproceedings {caerus,
author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Jingrong Chen and Ion Stoica},
title = {Caerus: {NIMBLE} Task Scheduling for Serverless Analytics},
booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
year = {2021},
isbn = {978-1-939133-21-2},
pages = {653--669},
url = {https://www.usenix.org/conference/nsdi21/presentation/zhang-hong},
publisher = {USENIX Association},
month = apr
}

@inproceedings {shepherd,
author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica},
title = {{SHEPHERD}: Serving {DNNs} in the Wild},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {787--808},
url = {https://www.usenix.org/conference/nsdi23/presentation/zhang-hong},
publisher = {USENIX Association},
month = apr
}

@inproceedings{jiffy,
author = {Khandelwal, Anurag and Tang, Yupeng and Agarwal, Rachit and Akella, Aditya and Stoica, Ion},
title = {Jiffy: Elastic Far-Memory for Stateful Serverless Analytics},
year = {2022},
isbn = {9781450391627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492321.3527539},
doi = {10.1145/3492321.3527539},
abstract = {Stateful serverless analytics can be enabled using a remote memory system for inter-task communication, and for storing and exchanging intermediate data. However, existing systems allocate memory resources at job granularity---jobs specify their memory demands at the time of the submission; and, the system allocates memory equal to the job's demand for the entirety of its lifetime. This leads to resource underutilization and/or performance degradation when intermediate data sizes vary during job execution.This paper presents Jiffy, an elastic far-memory system for stateful serverless analytics that meets the instantaneous memory demand of a job at seconds timescales. Jiffy efficiently multiplexes memory capacity across concurrently running jobs, reducing the overheads of reads and writes to slower persistent storage, resulting in 1.6 -- 2.5\texttimes{} improvements in job execution time over production workloads. Jiffy implementation currently runs on Amazon EC2, enables a wide variety of distributed programming models including MapReduce, Dryad, StreamScope, and Piccolo, and natively supports a large class of analytics applications on AWS Lambda.},
booktitle = {Proceedings of the Seventeenth European Conference on Computer Systems},
pages = {697–713},
numpages = {17},
keywords = {function-as-a-service, serverless computing, far-memory, data analytics},
location = {Rennes, France},
series = {EuroSys '22}
}

@misc{chase,
title={{CHASE: Accelerating Distributed Pointer-Traversals on Disaggregated Memory}},
year={2023},
howpublished="\url{https://arxiv.org/pdf/2305.02388.pdf}",
}

@misc{dataintensive,
  title = "The Cloud Native Convergence: A New Era of Data-Intensive Applications", 
  author = "Dez Blanchfield", 
  howpublished = {\url{https://elnion.com/2023/06/05/the-cloud-native-convergence-a-new-era-of-data-intensive-applications/}}
}


@article{nvlink,
  title={Evaluating modern gpu interconnect: Pcie, nvlink, nv-sli, nvswitch and gpudirect},
  author={Li, Ang and Song, Shuaiwen Leon and Chen, Jieyang and Li, Jiajia and Liu, Xu and Tallent, Nathan R and Barker, Kevin J},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={31},
  number={1},
  pages={94--110},
  year={2019},
  publisher={IEEE}
}

@inproceedings {nyxcache,
author = {Kan Wu and Kaiwei Tu and Yuvraj Patel and Rathijit Sen and Kwanghyun Park and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},
title = {{NyxCache}: Flexible and Efficient Multi-tenant Persistent Memory Caching},
booktitle = {20th USENIX Conference on File and Storage Technologies (FAST 22)},
year = {2022},
isbn = {978-1-939133-26-7},
address = {Santa Clara, CA},
pages = {1--16},
url = {https://www.usenix.org/conference/fast22/presentation/wu},
publisher = {USENIX Association},
month = feb,
}

@inproceedings{numalloc,
author = {Yang, Hanmei and Zhao, Xin and Zhou, Jin and Wang, Wei and Kundu, Sandip and Wu, Bo and Guan, Hui and Liu, Tongping},
title = {NUMAlloc: A Faster NUMA Memory Allocator},
year = {2023},
isbn = {9798400701795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591195.3595276},
doi = {10.1145/3591195.3595276},
abstract = {The NUMA architecture accommodates the hardware trend of an increasing number of CPU cores. It requires the cooperation of memory allocators to achieve good performance for multithreaded applications. Unfortunately, existing allocators do not support NUMA architecture well. This paper presents a novel memory allocator – NUMAlloc, that is designed for the NUMA architecture. is centered on a binding-based memory management. On top of it, proposes an “origin-aware memory management” to ensure the locality of memory allocations and deallocations, as well as a method called “incremental sharing” to balance the performance benefits and memory overhead of using transparent huge pages. According to our extensive evaluation, NUMAlloc has the best performance among all evaluated allocators, running 15.7\% faster than the second-best allocator (mimalloc), and 20.9\% faster than the default Linux allocator with reasonable memory overhead. NUMAlloc is also scalable to 128 threads and is ready for deployment.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on Memory Management},
pages = {97–110},
numpages = {14},
keywords = {NUMA Architecture, Memory Allocation},
location = {Orlando, FL, USA},
series = {ISMM 2023}
}

@inproceedings {empiricalPM,
author = {Jian Yang and Juno Kim and Morteza Hoseinzadeh and Joseph Izraelevitz and Steve Swanson},
title = {An Empirical Guide to the Behavior and Use of Scalable Persistent Memory},
booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
year = {2020},
isbn = {978-1-939133-12-0},
address = {Santa Clara, CA},
pages = {169--182},
url = {https://www.usenix.org/conference/fast20/presentation/yang},
publisher = {USENIX Association},
month = feb,
}

@misc(EPYC,
  title = {{AMD. AMD EPYC 9004 Series Server Processors}},
  howpublished = "\url{https://www.amd.com/en/processors/epyc-9004-series}",
)

@misc{spark,
    title = {{Apache Spark. Unified engine for large-scale data analytics.}},
    howpublished = "\url{https://spark.apache.org/}"},
}

@misc{H100,
    title = {{NVIDIA. NVIDIA H100 Tensor Core GPU}},
    howpublished = "\url{https://www.nvidia.com/en-gb/data-center/h100/}"},
}

@INPROCEEDINGS{amx,
  author={Nassif, Nevine and Munch, Ashley O. and Molnar, Carleton L. and Pasdast, Gerald and Lyer, Sitaraman V. and Yang, Zibing and Mendoza, Oscar and Huddart, Mark and Venkataraman, Srikrishnan and Kandula, Sireesha and Marom, Rafi and Kern, Alexandra M. and Bowhill, Bill and Mulvihill, David R. and Nimmagadda, Srikanth and Kalidindi, Varma and Krause, Jonathan and Haq, Mohammad M. and Sharma, Roopali and Duda, Kevin},
  booktitle={2022 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  title={Sapphire Rapids: The Next-Generation Intel Xeon Scalable Processor}, 
  year={2022},
  volume={65},
  number={},
  pages={44-46},
  doi={10.1109/ISSCC42614.2022.9731107}}


@misc{cpuinference,
    title = {{Timothy P. Morgan. WHY AI INFERENCE WILL REMAIN LARGELY ON THE CPU}},
    howpublished = "\url{https://www.nextplatform.com/2023/04/05/why-ai-inference-will-remain-largely-on-the-cpu/}",
}

@misc{intelbenchmark,
    title = {{Intel Corporation. 4th Generation Intel® Xeon® Scalable Processors Performance.}},
    howpublished = "\url{https://edc.intel.com/content/www/us/en/products/performance/benchmarks/4th-generation-intel-xeon-scalable-processors/}",
}

@misc{amdepyc,
    title={{Videocardz. AMD next-gen socket SP7 reportedly launches alongside Venice data-center CPUs}},
    howpublished = "\url{https://videocardz.com/newz/amd-epyc-venice-with-zen6-cpu-cores-reportedly-uses-new-sp7-socket}",
}

@misc{redis-benchmark,
    title={{Redis benchmark. Using the redis-benchmark utility on a Redis server.}},
    howpublished = "\url{https://lore.kernel.org/lkml/CAAPL-u9Wv+nH1VOZTj=9p9S70Y3Qz3+63EkqncRDdHfubsrjfw@mail.gmail.com/}",
}

@misc{intelsnc,
    title={{Intel Xeon Processor Scalable Family Technical Overview.}},
    howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html}",
}

@misc{gh200,
title={{Nvidia GH200 Datasheet.}},
howpublished="\url{https://resources.nvidia.com/en-us-dgx-gh200/nvidia-grace-hopper-superchip-datasheet}",
}
@misc{appleuma,
title={{Apple Introduces M2 Ultra.}},
howpublished="\url{https://www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/}",
}
@misc{pcieopt,
title={{PCI-SIG explores an optical interconnect for higher PCIe performance}},
howpublished="\url{https://www.eenewseurope.com/en/pci-sig-explores-an-optical-connections-for-higher-pcie-performance/}",
}
@misc{uec,
title={{Ultra Ethernet Consortium.}},
howpublished="\url{https://ultraethernet.org/}",
}

@inproceedings{tpuv4,
author = {N. Jouppi and et al.},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
}


@inproceedings{moe-iclr17,
author = {N. Shazeer and et al.},
title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
year = {2017},
booktitle = {The 5th International Conference on Learning Representations},
}



@INPROCEEDINGS{snoopfilter,
  author={Sharma, Debendra Das},
  booktitle={2022 IEEE Symposium on High-Performance Interconnects (HOTI)}, 
  title={Compute Express Link®: An open industry-standard interconnect enabling heterogeneous data-centric computing}, 
  year={2022},
  volume={},
  number={},
  pages={5-12},
  doi={10.1109/HOTI55740.2022.00017}}

@misc{h100amazon,
title={{Nvidia H100 80GB on Amazon.}},
year={2023},
howpublished="\url{https://www.amazon.com/Tesla-NVIDIA-Learning-Compute-Graphics/dp/B0C3XH4QSJ}",
}

@misc{msssd,
title={{Samsung Memory Semantic SSD.}},
year={2023},
howpublished="\url{https://samsungmsl.com/ms-ssd/}",
}

@misc{inferllm,
title={{InferLLM: a lightweight LLM model inference framework}},
howpublished = "\url{https://github.com/MegEngine/InferLLM}",
}

@misc{tpch,
title={{TPC-H is a Decision Support Benchmark}},
howpublished = "\url{https://www.tpc.org/tpch/}",
}

@misc{kvcache,
      title={Efficiently Scaling Transformer Inference}, 
      author={Reiner Pope and Sholto Douglas and Aakanksha Chowdhery and Jacob Devlin and James Bradbury and Anselm Levskaya and Jonathan Heek and Kefan Xiao and Shivani Agrawal and Jeff Dean},
      year={2022},
      eprint={2211.05102},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{vllmpaper,
author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613165},
doi = {10.1145/3600006.3613165},
abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {611–626},
numpages = {16},
location = {Koblenz, Germany},
series = {SOSP '23}
}

@misc{sharma2023introduction,
      title={An Introduction to the Compute Express Link (CXL) Interconnect}, 
      author={Debendra Das Sharma and Robert Blankenship and Daniel S. Berger},
      year={2023},
      eprint={2306.11227},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}

@misc{pcie5.0,
title={{What Are PCIe 4.0 and 5.0?}},
howpublished="\url{https://www.intel.com/content/www/us/en/gaming/resources/what-is-pcie-4-and-why-does-it-matter.html}",
}

@misc{keyDB,
title={{keyDB:A Multithreaded Fork of Redis}},
howpublished="\url{https://github.com/Snapchat/KeyDB}",
}

@misc{alpaca,
title={{Alpaca: A Strong, Replicable Instruction-Following Model}},
howpublished="\url{https://crfm.stanford.edu/2023/03/13/alpaca.html}",
}

@inproceedings {pmem,
author = {Jian Yang and Juno Kim and Morteza Hoseinzadeh and Joseph Izraelevitz and Steve Swanson},
title = {An Empirical Guide to the Behavior and Use of Scalable Persistent Memory},
booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
year = {2020},
isbn = {978-1-939133-12-0},
address = {Santa Clara, CA},
pages = {169--182},
url = {https://www.usenix.org/conference/fast20/presentation/yang},
publisher = {USENIX Association},
month = feb
}

@ARTICLE{CXLPoolCost,
  author={Berger, Daniel S. and Ernst, Daniel and Li, Huaicheng and Zardoshti, Pantea and Shah, Monish and Rajadnya, Samir and Lee, Scott and Hsu, Lisa and Agarwal, Ishwar and Hill, Mark D. and Bianchini, Ricardo},
  journal={IEEE Micro}, 
  title={Design Tradeoffs in CXL-Based Memory Pools for Public Cloud Platforms}, 
  year={2023},
  volume={43},
  number={2},
  pages={30-38},
  keywords={Servers;Memory management;Cloud computing;Bandwidth;Random access memory;Costs;Hardware},
  doi={10.1109/MM.2023.3241586}}

@misc{icelakecores,
title={{Ice Lake SP: Overview and technical documentation. (n.d.). Intel.}},
howpublished="\url{https://www.intel.com/content/www/us/en/products/platforms/details/ice-lake-sp.html}",
}


@misc{sprcores,
title={{ 4th Gen Intel Xeon Processor Scalable Family, sapphire rapids. (n.d.). Intel.}},
howpublished="\url{https://www.intel.com/content/www/us/en/developer/articles/technical/fourth-generation-xeon-scalable-family-overview.html#gs.3m5uv2}",
}

@misc{sierraforestcores,
title={{Kennedy, Patrick. “Intel Shows Granite Rapids and Sierra Forest Motherboards at OCP Summit 2023.” ServeTheHome, 26 Oct. 2023,}},
howpublished="\url{www.servethehome.com/intel-shows-granite-rapids-and-sierra-forest-motherboards-at-ocp-summit-2023-qct-wistron}",
}


@misc{emeraldrapidscores,
title={{McDowell, S. (2023, December 18). Intel launches 5th generation “Emerald Rapids” Xeon processors. Forbes}},
howpublished="\url{https://www.forbes.com/sites/stevemcdowell/2023/12/17/intel-launches-5th-generation-emerald-rapids-xeon-processors/}",
}

@misc{clearwatercores,
title={{Mujtaba, H. (2023, December 1). Intel Clearwater Forest E-Core Only Xeon CPUs to offer up to 288 cores}},
howpublished="\url{https://wccftech.com/intel-clearwater-forest-e-core-xeon-cpus-up-to-288-cores-higher-ipc-more-cache/}",
}

@misc{amdgenoabergamo,
title={{AMD Unveils Zen 4 CPU Roadmap: 96-Core 5nm Genoa in 2022, 128-Core Bergamo in 2023.}},
howpublished="\url{https://wccftech.com/intel-clearwater-forest-e-core-xeon-cpus-up-to-288-cores-higher-ipc-more-cache/}",
}

@misc{micron,
title={{CZ120 memory expansion module.}},
howpublished="\url{https://www.micron.com/products/memory/cxl-memory}",
}

@misc{volcano,
title={{Elastic Compute Service, Volcano Engine, Bytedance}},
howpublished="\url{https://www.volcengine.com/product/ecs}",
}

@misc{1dpc,
title={{Intel Shows Granite Rapids and Sierra Forest Motherboards at OCP Summit 2023}},
howpublished="\url{https://www.servethehome.com/intel-shows-granite-rapids-and-sierra-forest-motherboards-at-ocp-summit-2023-qct-wistron/}",
}


@inproceedings{cxl1,
author = {Sun, Yan and Yuan, Yifan and Yu, Zeduo and Kuper, Reese and Song, Chihun and Huang, Jinghan and Ji, Houxiang and Agarwal, Siddharth and Lou, Jiaqi and Jeong, Ipoom and Wang, Ren and Ahn, Jung Ho and Xu, Tianyin and Kim, Nam Sung},
title = {Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices},
year = {2023},
isbn = {9798400703294},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3613424.3614256},
doi = {10.1145/3613424.3614256},
abstract = {The ever-growing demands for memory with larger capacity and higher bandwidth have driven recent innovations on memory expansion and disaggregation technologies based on Compute eXpress Link (CXL). Especially, CXL-based memory expansion technology has recently gained notable attention for its ability not only to economically expand memory capacity and bandwidth but also to decouple memory technologies from a specific memory interface of the CPU. However, since CXL memory devices have not been widely available, they have been emulated using DDR memory in a remote NUMA node. In this paper, for the first time, we comprehensively evaluate a true CXL-ready system based on the latest 4th-generation Intel Xeon CPU with three CXL memory devices from different manufacturers. Specifically, we run a set of microbenchmarks not only to compare the performance of true CXL memory with that of emulated CXL memory but also to analyze the complex interplay between the CPU and CXL memory in depth. This reveals important differences between emulated CXL memory and true CXL memory, some of which will compel researchers to revisit the analyses and proposals from recent work. Next, we identify opportunities for memory-bandwidth-intensive applications to benefit from the use of CXL memory. Lastly, we propose a CXL-memory-aware dynamic page allocation policy, Caption to more efficiently use CXL memory as a bandwidth expander. We demonstrate that Caption can automatically converge to an empirically favorable percentage of pages allocated to CXL memory, which improves the performance of memory-bandwidth-intensive applications by up to 24\% when compared to the default page allocation policy designed for traditional NUMA systems.},
booktitle = {Proceedings of the 56th Annual IEEE/ACM International Symposium on Microarchitecture},
pages = {105–121},
numpages = {17},
keywords = {tiered-memory management, measurement, Compute eXpress Link},
location = {Toronto, ON, Canada},
series = {MICRO '23}
}

@inproceedings{cxl2,
author = {Tang, Yupeng and Zhou, Ping and Zhang, Wenhui and Hu, Henry and Yang, Qirui and Xiang, Hao and Liu, Tongping and Shan, Jiaxin and Huang, Ruoyun and Zhao, Cheng and Chen, Cheng and Zhang, Hui and Liu, Fei and Zhang, Shuai and Ding, Xiaoning and Chen, Jianjun},
title = {Exploring Performance and Cost Optimization with ASIC-Based CXL Memory},
year = {2024},
isbn = {9798400704376},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627703.3650061},
doi = {10.1145/3627703.3650061},
abstract = {As memory-intensive applications continue to drive the need for advanced architectural solutions, Compute Express Link (CXL) has risen as a promising interconnect technology that enables seamless high-speed, low-latency communication between host processors and various peripheral devices. In this study, we explore the application performance of ASIC CXL memory in various data-center scenarios. We then further explore multiple potential impacts (e.g., throughput, latency, and cost reduction) of employing CXL memory via carefully designed policies and strategies. Our empirical results show the high potential of CXL memory, reveal multiple intriguing observations of CXL memory and contribute to the wide adoption of CXL memory in real-world deployment environments. Based on our benchmarks, we also develop an Abstract Cost Model that can estimate the cost benefit from using CXL memory.},
booktitle = {Proceedings of the Nineteenth European Conference on Computer Systems},
pages = {818–833},
numpages = {16},
keywords = {CXL-Memory, Datacenters, Memory Management, Operating Systems, measurement},
location = {Athens, Greece},
series = {EuroSys '24}
}

@inproceedings {distserve,
author = {Yinmin Zhong and Shengyu Liu and Junda Chen and Jianbo Hu and Yibo Zhu and Xuanzhe Liu and Xin Jin and Hao Zhang},
title = {{DistServe}: Disaggregating Prefill and Decoding for Goodput-optimized Large Language Model Serving},
booktitle = {18th USENIX Symposium on Operating Systems Design and Implementation (OSDI 24)},
year = {2024},
isbn = {978-1-939133-40-3},
address = {Santa Clara, CA},
pages = {193--210},
url = {https://www.usenix.org/conference/osdi24/presentation/zhong-yinmin},
publisher = {USENIX Association},
month = jul
}

@misc{memserve,
      title={MemServe: Context Caching for Disaggregated LLM Serving with Elastic Memory Pool}, 
      author={Cunchen Hu and Heyang Huang and Junhao Hu and Jiang Xu and Xusheng Chen and Tao Xie and Chenxi Wang and Sa Wang and Yungang Bao and Ninghui Sun and Yizhou Shan},
      year={2024},
      eprint={2406.17565},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2406.17565}, 
}

@inproceedings{pagedattenion,
author = {Kwon, Woosuk and Li, Zhuohan and Zhuang, Siyuan and Sheng, Ying and Zheng, Lianmin and Yu, Cody Hao and Gonzalez, Joseph and Zhang, Hao and Stoica, Ion},
title = {Efficient Memory Management for Large Language Model Serving with PagedAttention},
year = {2023},
isbn = {9798400702297},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3600006.3613165},
doi = {10.1145/3600006.3613165},
abstract = {High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2--4\texttimes{} with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm.},
booktitle = {Proceedings of the 29th Symposium on Operating Systems Principles},
pages = {611–626},
numpages = {16},
location = {Koblenz, Germany},
series = {SOSP '23}
}



@misc{kvcost,
title = {{LLM Inference Series: 4. KV caching, a deeper look}},
author = {Pierre Lienhart},
year = {2024},
howpublished = "\url{https://medium.com/@plienhar/llm-inference-series-4-kv-caching-a-deeper-look-4ba9a77746c8}"
}

@article{gpt1,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{gpt2,
  title={GPT-3: Its nature, scope, limits, and consequences},
  author={Floridi, Luciano and Chiriatti, Massimo},
  journal={Minds and Machines},
  volume={30},
  pages={681--694},
  year={2020},
  publisher={Springer}
}
@article{gpt3,
  title={How contextual are contextualized word representations? Comparing the geometry of BERT, ELMo, and GPT-2 embeddings},
  author={Ethayarajh, Kawin},
  journal={arXiv preprint arXiv:1909.00512},
  year={2019}
}

@misc{cacheblend,
      title={CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion}, 
      author={Jiayi Yao and Hanchen Li and Yuhan Liu and Siddhant Ray and Yihua Cheng and Qizheng Zhang and Kuntai Du and Shan Lu and Junchen Jiang},
      year={2024},
      eprint={2405.16444},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.16444}, 
}



@misc{intelplatinum,
title = {{Intel® Xeon® Platinum Processor}},
author = {Intel Corporation},
year = {2024},
howpublished = "\url{https://www.intel.com/content/www/us/en/products/details/processors/xeon/scalable/platinum.html}"
}

@article{llama2,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{cudaapi,
title = {{CUDA Runtime API - Memory Management Functions}},
author = {NVIDIA},
year={2024},
howpublished="\url{https://docs.nvidia.com/cuda/cuda-runtime-api/group\_\_CUDART\_\_MEMORY.html}"
}

@misc{slo,
      title={Aladdin: Joint Placement and Scaling for SLO-Aware LLM Serving}, 
      author={Chengyi Nie and Rodrigo Fonseca and Zhenhua Liu},
      year={2024},
      eprint={2405.06856},
      archivePrefix={arXiv},
      primaryClass={cs.DC},
      url={https://arxiv.org/abs/2405.06856}, 
}

@inproceedings{cxlgpu1,
author = {Arif, Moiz and Maurya, Avinash and Rafique, M. Mustafa},
title = {Accelerating Performance of GPU-based Workloads Using CXL},
year = {2023},
isbn = {9798400701665},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3589013.3596678},
doi = {10.1145/3589013.3596678},
abstract = {High-performance computing (HPC) workloads such as scientific simulations and deep learning (DL) running across multi-GPU systems are memory and data-intensive, relying on the main memory to complement its limited onboard high-bandwidth memory (HBM). To facilitate faster data transfer across the slow device-to-host PCIe interconnects, these workloads typically pin memory on the host system, thereby creating a memory capacity limitation on the host memory for workloads running on peer GPUs of the same node. Compute express link (CXL) is an emerging technology that transparently extends the available system memory capacity at low latency and high throughput in a cache-coherent fashion. While this can be leveraged by workloads running across multi-GPU nodes to allocate and pin more memory, using conventional memory allocation schemes can adversely impact the data throughput due to contention on the CXL memory. To this end, we highlight the challenges related to conventional job scheduling and memory allocation on such CXL-enabled multi-GPU systems and propose an algorithm to mitigate the contention on the CXL memory, maximize throughput and reduce the overall data transfer time. Our preliminary evaluation of our proposed memory allocation approach based on simulations of a variety of job profiles and system configurations demonstrates up to 65\% lower data transfer overheads as compared to the existing memory allocation approaches.},
booktitle = {Proceedings of the 13th Workshop on AI and Scientific Computing at Scale Using Flexible Computing},
pages = {27–31},
numpages = {5},
keywords = {compute express link (CXL), memory allocation, multi-GPU systems, pinned memory, tiered memory},
location = {Orlando, FL, USA},
series = {FlexScience '23}
}

@inproceedings{cxlgpu2,
author = {Sano, Shintaro and Bando, Yosuke and Hiwada, Kazuhiro and Kajihara, Hirotsugu and Suzuki, Tomoya and Nakanishi, Yu and Taki, Daisuke and Kaneko, Akiyuki and Shiozawa, Tatsuo},
title = {GPU Graph Processing on CXL-Based Microsecond-Latency External Memory},
year = {2023},
isbn = {9798400707858},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3624062.3624173},
doi = {10.1145/3624062.3624173},
abstract = {In GPU graph analytics, the use of external memory such as the host DRAM and solid-state drives is a cost-effective approach to processing large graphs beyond the capacity of the GPU onboard memory. This paper studies the use of Compute Express Link (CXL) memory as alternative external memory for GPU graph processing in order to see if this emerging memory expansion technology enables graph processing that is as fast as using the host DRAM. Through analysis and evaluation using FPGA prototypes, we show that representative GPU graph traversal algorithms involving fine-grained random access can tolerate an external memory latency of up to a few microseconds introduced by the CXL interface as well as by the underlying memory devices. This insight indicates that microsecond-latency flash memory may be used as CXL memory devices to realize even more cost-effective GPU graph processing while still achieving performance close to using the host DRAM.},
booktitle = {Proceedings of the SC '23 Workshops of The International Conference on High Performance Computing, Network, Storage, and Analysis},
pages = {962–972},
numpages = {11},
keywords = {CXL, GPU, flash, graph, latency, memory},
location = {Denver, CO, USA},
series = {SC-W '23}
}
@inproceedings{cxlgpu3,
author = {Gouk, Donghyun and Kang, Seungkwan and Bae, Hanyeoreum and Ryu, Eojin and Lee, Sangwon and Kim, Dongpyung and Jang, Junhyeok and Jung, Myoungsoo},
title = {Breaking Barriers: Expanding GPU Memory with Sub-Two Digit Nanosecond Latency CXL Controller},
year = {2024},
isbn = {9798400706301},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3655038.3665953},
doi = {10.1145/3655038.3665953},
abstract = {This work introduces a GPU storage expansion solution utilizing CXL, featuring a novel GPU system design with multiple CXL root ports for integrating diverse storage media (DRAMs and/or SSDs). We developed and siliconized a custom CXL controller integrated at the hardware RTL level, achieving two-digit nanosecond roundtrip latency, the first in the field. This study also includes speculative read and deterministic store mechanisms to efficiently manage read and write operations to hide the endpoint's backend media latency variation. Performance evaluations reveal our approach significantly outperforms existing methods, marking a substantial advancement in GPU storage technology.},
booktitle = {Proceedings of the 16th ACM Workshop on Hot Topics in Storage and File Systems},
pages = {108–115},
numpages = {8},
keywords = {CXL, GPGPU, Memory Expansion},
location = {Santa Clara, CA, USA},
series = {HotStorage '24}
}
@misc{dataset,
title = {{ShareGPT Vicuna unfiltered dataset}},
author = {anon8231489123},
year = {2024},
howpublished = "\url{https://huggingface.co/datasets/anon8231489123/ShareGPT_Vicuna_unfiltered}"
}

@misc{gpt-fast,
title = {{GPT-fast Simple and efficient pytorch-native transformer text generation.}},
author = {PyTorch Labs},
year = {2024},
howpublished = "\url{https://github.com/pytorch-labs/gpt-fast.git}"
}
@misc{ttft,
title = {{Llama 2 70B: An MLPerf Inference Benchmark for Large Language Models.}},
author = {Meta},
year = {2024},
howpublished = "\url{https://mlcommons.org/2024/03/mlperf-llama2-70b}"
}

@article{agrawal2023sarathi,
  title={Sarathi: Efficient llm inference by piggybacking decodes with chunked prefills},
  author={Agrawal, Amey and Panwar, Ashish and Mohan, Jayashree and Kwatra, Nipun and Gulavani, Bhargav S and Ramjee, Ramachandran},
  journal={arXiv preprint arXiv:2308.16369},
  year={2023}
}

@inproceedings{patel2024splitwise,
  title={Splitwise: Efficient generative llm inference using phase splitting},
  author={Patel, Pratyush and Choukse, Esha and Zhang, Chaojie and Shah, Aashaka and Goiri, {\'I}{\~n}igo and Maleki, Saeed and Bianchini, Ricardo},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={118--132},
  year={2024},
  organization={IEEE}
}


@article{miao2023towards,
  title={Towards efficient generative large language model serving: A survey from algorithms to systems},
  author={Miao, Xupeng and Oliaro, Gabriele and Zhang, Zhihao and Cheng, Xinhao and Jin, Hongyi and Chen, Tianqi and Jia, Zhihao},
  journal={arXiv preprint arXiv:2312.15234},
  year={2023}
}

@article{jiang2023mistral,
  title={Mistral 7B},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and Casas, Diego de las and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and others},
  journal={arXiv preprint arXiv:2310.06825},
  year={2023}
}

@inproceedings{leviathan2023fast,
  title={Fast inference from transformers via speculative decoding},
  author={Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  booktitle={International Conference on Machine Learning},
  pages={19274--19286},
  year={2023},
  organization={PMLR}
}

@inproceedings{narayanan2021efficient,
  title={Efficient large-scale language model training on gpu clusters using megatron-lm},
  author={Narayanan, Deepak and Shoeybi, Mohammad and Casper, Jared and LeGresley, Patrick and Patwary, Mostofa and Korthikanti, Vijay and Vainbrand, Dmitri and Kashinkunti, Prethvi and Bernauer, Julie and Catanzaro, Bryan and others},
  booktitle={Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis},
  pages={1--15},
  year={2021}
}


@inproceedings{jacob2018quantization,
  title={Quantization and training of neural networks for efficient integer-arithmetic-only inference},
  author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu, Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and Kalenichenko, Dmitry},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2704--2713},
  year={2018}
}


@inproceedings{sheng2023flexgen,
  title={Flexgen: High-throughput generative inference of large language models with a single gpu},
  author={Sheng, Ying and Zheng, Lianmin and Yuan, Binhang and Li, Zhuohan and Ryabinin, Max and Chen, Beidi and Liang, Percy and R{\'e}, Christopher and Stoica, Ion and Zhang, Ce},
  booktitle={International Conference on Machine Learning},
  pages={31094--31116},
  year={2023},
  organization={PMLR}
}

@article{liu2023cachegen,
  title={Cachegen: Fast context loading for language model applications},
  author={Liu, Yuhan and Li, Hanchen and Du, Kuntai and Yao, Jiayi and Cheng, Yihua and Huang, Yuyang and Lu, Shan and Maire, Michael and Hoffmann, Henry and Holtzman, Ari and others},
  journal={arXiv preprint arXiv:2310.07240},
  year={2023}
}
@article{pcie,
  title={Evaluating modern gpu interconnect: Pcie, nvlink, nv-sli, nvswitch and gpudirect},
  author={Li, Ang and Song, Shuaiwen Leon and Chen, Jieyang and Li, Jiajia and Liu, Xu and Tallent, Nathan R and Barker, Kevin J},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={31},
  number={1},
  pages={94--110},
  year={2019},
  publisher={IEEE}
}



@inproceedings{zhong2024managing,
author = {Zhong, Yuhong and Berger, Daniel S. and Waldspurger, Carl and Agarwal, Ishwar and Agarwal, Rajat and Hady, Frank and Kumar, Karthik and Hill, Mark D. and Chowdhury, Mosharaf and Cidon, Asaf},
title = {Managing Memory Tiers with CXL in Virtualized Environments},
booktitle = {USENIX OSDI},
year = {2024},
month = {July},
abstract = {Cloud providers seek to deploy CXL-based memory to increase aggregate memory capacity, reduce costs, and lower carbon emissions. However, CXL accesses incur higher latency than local DRAM. Existing systems use software to manage data placement across memory tiers at page granularity. Cloud providers are reluctant to deploy software-based tiering due to high overheads in virtualized environments. Hardware-based memory tiering could place data at cacheline granularity, mitigating these drawbacks. However, hardware is oblivious to application-level performance.

We propose combining hardware-managed tiering with software-managed performance isolation to overcome the pitfalls of either approach. We introduce Intel® Flat Memory Mode, the first hardware-managed tiering system for CXL. Our evaluation on a full-system prototype demonstrates that it provides performance close to regular DRAM, with no more than 5% degradation for more than 82% of workloads. Despite such small slowdowns, we identify two challenges that can still degrade performance by up to 34% for "outlier" workloads: (1) memory contention across tenants, and (2) intra-tenant contention due to conflicting access patterns.

To address these challenges, we introduce Memstrata, a lightweight multi-tenant memory allocator. Memstrata employs page coloring to eliminate inter-VM contention. It improves performance for VMs with access patterns that are sensitive to hardware tiering by allocating them more local DRAM using an online slowdown estimator. In multi-VM experiments on prototype hardware, Memstrata is able to identify performance outliers and reduce their degradation from above 30% to below 6%, providing consistent performance across a wide range of workloads.},
url = {https://www.microsoft.com/en-us/research/publication/managing-memory-tiers-with-cxl-in-virtualized-environments/},
}