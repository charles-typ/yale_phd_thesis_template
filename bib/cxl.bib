

@misc{cxl,
  title        = {{Compute Express Link (CXL)}},
  howpublished = {\url{https://www.computeexpresslink.org/}}
}

@article{cxl_azure,
  title={First-generation Memory Disaggregation for Cloud Platforms},
  author={Li, Huaicheng and Berger, Daniel S and Novakovic, Stanko and Hsu, Lisa and Ernst, Dan and Zardoshti, Pantea and Shah, Monish and Agarwal, Ishwar and Hill, Mark and Fontoura, Marcus and others},
  journal={arXiv preprint arXiv:2203.00241},
  year={2022}
}

@misc{cxlcentric,
      title={A Case for CXL-Centric Server Processors}, 
      author={Albert Cho and Anish Saxena and Moinuddin Qureshi and Alexandros Daglis},
      year={2023},
      eprint={2305.05033},
      archivePrefix={arXiv},
      primaryClass={cs.AR}
}


@article{pond,
  title={Pond: CXL-Based Memory Pooling Systems for Cloud Platforms},
  author={Huaicheng Li and Daniel S. Berger and Stanko Novakovic and Lisa R. Hsu and Dan Ernst and Pantea Zardoshti and Monish Shah and Samir Rajadnya and Scott Lee and Ishwar Agarwal and Mark D. Hill and Marcus Fontoura and Ricardo Bianchini},
  journal={Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  year={2022},
  url={https://api.semanticscholar.org/CorpusID:252907213}
}

@inproceedings{tpp,
author = {Maruf, Hasan Al and Wang, Hao and Dhanotia, Abhishek and Weiner, Johannes and Agarwal, Niket and Bhattacharya, Pallab and Petersen, Chris and Chowdhury, Mosharaf and Kanaujia, Shobhit and Chauhan, Prakash},
title = {TPP: Transparent Page Placement for CXL-Enabled Tiered-Memory},
year = {2023},
isbn = {9781450399180},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3582016.3582063},
doi = {10.1145/3582016.3582063},
abstract = {The increasing demand for memory in hyperscale applications has led to memory becoming a large portion of the overall datacenter spend. The emergence of coherent interfaces like CXL enables main memory expansion and offers an efficient solution to this problem. In such systems, the main memory can constitute different memory technologies with varied characteristics. In this paper, we characterize memory usage patterns of a wide range of datacenter applications across the server fleet of Meta. We, therefore, demonstrate the opportunities to offload colder pages to slower memory tiers for these applications. Without efficient memory management, however, such systems can significantly degrade performance. We propose a novel OS-level application-transparent page placement mechanism (TPP) for CXL-enabled memory. TPP employs a lightweight mechanism to identify and place hot/cold pages to appropriate memory tiers. It enables a proactive page demotion from local memory to CXL-Memory. This technique ensures a memory headroom for new page allocations that are often related to request processing and tend to be short-lived and hot. At the same time, TPP can promptly promote performance-critical hot pages trapped in the slow CXL-Memory to the fast local memory, while minimizing both sampling overhead and unnecessary migrations. TPP works transparently without any application-specific knowledge and can be deployed globally as a kernel release. We evaluate TPP with diverse memory-sensitive workloads in the production server fleet with early samples of new x86 CPUs with CXL 1.1 support. TPP makes a tiered memory system performant as an ideal baseline (&lt;1\% gap) that has all the memory in the local tier. It is 18\% better than today’s Linux, and 5–17\% better than existing solutions including NUMA Balancing and AutoTiering. Most of the TPP patches have been merged in the Linux v5.18 release while the remaining ones are just pending for more discussion.},
booktitle = {Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3},
pages = {742–755},
numpages = {14},
keywords = {Operating Systems, Memory Management, Datacenters, Tiered-Memory, Heterogeneous System, CXL-Memory},
location = {Vancouver, BC, Canada},
series = {ASPLOS 2023}
}

@inproceedings {mt2,
author = {Jifei Yi and Benchao Dong and Mingkai Dong and Ruizhe Tong and Haibo Chen},
title = {{MT\^2}: Memory Bandwidth Regulation on Hybrid {NVM/DRAM} Platforms},
booktitle = {20th USENIX Conference on File and Storage Technologies (FAST 22)},
year = {2022},
isbn = {978-1-939133-26-7},
address = {Santa Clara, CA},
pages = {199--216},
url = {https://www.usenix.org/conference/fast22/presentation/yi-mt2},
publisher = {USENIX Association},
month = feb,
}

@inproceedings {nyxcache,
author = {Kan Wu and Kaiwei Tu and Yuvraj Patel and Rathijit Sen and Kwanghyun Park and Andrea Arpaci-Dusseau and Remzi Arpaci-Dusseau},
title = {{NyxCache}: Flexible and Efficient Multi-tenant Persistent Memory Caching},
booktitle = {20th USENIX Conference on File and Storage Technologies (FAST 22)},
year = {2022},
isbn = {978-1-939133-26-7},
address = {Santa Clara, CA},
pages = {1--16},
url = {https://www.usenix.org/conference/fast22/presentation/wu},
publisher = {USENIX Association},
month = feb,
}

@inproceedings{numalloc,
author = {Yang, Hanmei and Zhao, Xin and Zhou, Jin and Wang, Wei and Kundu, Sandip and Wu, Bo and Guan, Hui and Liu, Tongping},
title = {NUMAlloc: A Faster NUMA Memory Allocator},
year = {2023},
isbn = {9798400701795},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3591195.3595276},
doi = {10.1145/3591195.3595276},
abstract = {The NUMA architecture accommodates the hardware trend of an increasing number of CPU cores. It requires the cooperation of memory allocators to achieve good performance for multithreaded applications. Unfortunately, existing allocators do not support NUMA architecture well. This paper presents a novel memory allocator – NUMAlloc, that is designed for the NUMA architecture. is centered on a binding-based memory management. On top of it, proposes an “origin-aware memory management” to ensure the locality of memory allocations and deallocations, as well as a method called “incremental sharing” to balance the performance benefits and memory overhead of using transparent huge pages. According to our extensive evaluation, NUMAlloc has the best performance among all evaluated allocators, running 15.7\% faster than the second-best allocator (mimalloc), and 20.9\% faster than the default Linux allocator with reasonable memory overhead. NUMAlloc is also scalable to 128 threads and is ready for deployment.},
booktitle = {Proceedings of the 2023 ACM SIGPLAN International Symposium on Memory Management},
pages = {97–110},
numpages = {14},
keywords = {NUMA Architecture, Memory Allocation},
location = {Orlando, FL, USA},
series = {ISMM 2023}
}

@ARTICLE{smt,
  author={Kim, Kyungsan and Kim, Hyunseok and So, Jinin and Lee, Wonjae and Im, Junhyuk and Park, Sungjoo and Cho, Jeonghyeon and Song, Hoyoung},
  journal={IEEE Micro}, 
  title={SMT: Software-Defined Memory Tiering for Heterogeneous Computing Systems With CXL Memory Expander}, 
  year={2023},
  volume={43},
  number={2},
  pages={20-29},
  doi={10.1109/MM.2023.3240774}}

@inproceedings {empiricalPM,
author = {Jian Yang and Juno Kim and Morteza Hoseinzadeh and Joseph Izraelevitz and Steve Swanson},
title = {An Empirical Guide to the Behavior and Use of Scalable Persistent Memory},
booktitle = {18th USENIX Conference on File and Storage Technologies (FAST 20)},
year = {2020},
isbn = {978-1-939133-12-0},
address = {Santa Clara, CA},
pages = {169--182},
url = {https://www.usenix.org/conference/fast20/presentation/yang},
publisher = {USENIX Association},
month = feb,
}

@ARTICLE{cxltradeoff,
  author={Berger, Daniel S. and Ernst, Daniel and Li, Huaicheng and Zardoshti, Pantea and Shah, Monish and Rajadnya, Samir and Lee, Scott and Hsu, Lisa and Agarwal, Ishwar and Hill, Mark D. and Bianchini, Ricardo},
  journal={IEEE Micro}, 
  title={Design Tradeoffs in CXL-Based Memory Pools for Public Cloud Platforms}, 
  year={2023},
  volume={43},
  number={2},
  pages={30-38},
  doi={10.1109/MM.2023.3241586}}

@misc{demystify,
      title={Demystifying CXL Memory with Genuine CXL-Ready Systems and Devices}, 
      author={Yan Sun and Yifan Yuan and Zeduo Yu and Reese Kuper and Ipoom Jeong and Ren Wang and Nam Sung Kim},
      year={2023},
      eprint={2303.15375},
      archivePrefix={arXiv},
      primaryClass={cs.PF}
}

@misc{Interleavepatch,
  title = {{J. Weiner. [PATCH] mm: mempolicy: N:M interleave policy for tiered memory nodes.}},
  howpublished = "\url{https://lore.kernel.org/linux-mm/YqD0%2FtzFwXvJ1gK6@cmpxchg.org/T/}",
}

@misc{mlc,
  title = {{V. Viswanathan, K. Kumar, and T. Willhalm. "Intel® Memory Latency Checker v3.10"}},
  howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/tool/intelr-memory-latency-checker.html}",
}

@inproceedings {directcxl,
author = {Donghyun Gouk and Sangwon Lee and Miryeong Kwon and Myoungsoo Jung},
title = {Direct Access, {High-Performance} Memory Disaggregation with {DirectCXL}},
booktitle = {2022 USENIX Annual Technical Conference (USENIX ATC 22)},
year = {2022},
isbn = {978-1-939133-29-65},
address = {Carlsbad, CA},
pages = {287--294},
url = {https://www.usenix.org/conference/atc22/presentation/gouk},
publisher = {USENIX Association},
month = jul,
}


@misc{A1000,
  title = {{Leo Memory Connectivity Platform for CXL 1.1 and 2.0}},
  howpublished = "\url{https://www.asteralabs.com/wp-content/uploads/2022/08/Astera_Labs_Leo_Aurora_Product_FINAL.pdf}",
}

@misc{SPR,
  title = {{Intel Corporation. Intel launches $\text{4}^{th}$ gen xeon scalable processors, max series cpus.}},
  howpublished = "\url{https://www.intel.com/content/www/us/en/newsroom/news/}",
}

@misc(EPYC,
  title = {{AMD. AMD EPYC 9004 Series Server Processors}},
  howpublished = "\url{https://www.amd.com/en/processors/epyc-9004-series}",
)

@misc{mxc,
  title = {{Montage Technology. Cxl memory expander controller (mxc).}},
  howpublished = "\url{https://www.montage-tech.com/MXC, accessed in 2023.}",
}

@misc{snc,
    title = {{David L Mulnix. Intel® Xeon® Processor Scalable Family Technical Overview}},
    howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html}",
}

@misc{intelfpga,
  title = {{Intel Corporation. Intel Agilex® 7 FPGA and SoC FPGA I-Series}},
  howpublished = "\url{https://www.intel.com/content/www/us/en/products/details/fpga/agilex/7/i-series.html}",
}

@misc{q8chat,
  title = {{Julien Simon.Smaller is Better: Q8-Chat LLM is an Efficient Generative AI Experience on Intel® Xeon® Processors}},
  howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/case-study/q8-chat-efficient-generative-ai-experience-xeon.html}",
}

@misc{emerald_rapids,
    title = {{Intel Corporation. Intel Unveils Future-Generation Xeon with Robust Performance and Efficiency Architectures}},
    howpublished = "\url{https://www.intel.com/content/www/us/en/newsroom/news/intel-unveils-future-generation-xeon.html}",
}

@misc{pcm,
    title = {{Intel Corporation. Intel® Performance Counter Monitor (Intel® PCM)}},
    howpublished = "\url{https://github.com/intel/pcm}",
}

@inproceedings{FlatFlash,
author = {Abulila, Ahmed and Mailthody, Vikram Sharma and Qureshi, Zaid and Huang, Jian and Kim, Nam Sung and Xiong, Jinjun and Hwu, Wen-mei},
title = {FlatFlash: Exploiting the Byte-Accessibility of SSDs within a Unified Memory-Storage Hierarchy},
year = {2019},
isbn = {9781450362405},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3297858.3304061},
doi = {10.1145/3297858.3304061},
booktitle = {Proceedings of the Twenty-Fourth International Conference on Architectural Support for Programming Languages and Operating Systems},
pages = {971–985},
numpages = {15},
keywords = {byte-addressable ssd, unified memory management, page promotion, data persistence},
location = {Providence, RI, USA},
series = {ASPLOS '19}
}

@inproceedings {cxl-ssd,
author = {Shao-Peng Yang and Minjae Kim and Sanghyun Nam and Juhyung Park and Jin-yong Choi and Eyee Hyun Nam and Eunji Lee and Sungjin Lee and Bryan S. Kim},
title = {Overcoming the Memory Wall with {CXL-Enabled} {SSDs}},
booktitle = {2023 USENIX Annual Technical Conference (USENIX ATC 23)},
year = {2023},
isbn = {978-1-939133-35-9},
address = {Boston, MA},
pages = {601--617},
url = {https://www.usenix.org/conference/atc23/presentation/yang-shao-peng},
publisher = {USENIX Association},
month = jul
}

@INPROCEEDINGS{PSACS,
  author={Zou, Chen and Zhang, Hui and Chien, Andrew A. and Seok Ki, Yang},
  booktitle={2021 IEEE 39th International Conference on Computer Design (ICCD)}, 
  title={PSACS: Highly-Parallel Shuffle Accelerator on Computational Storage}, 
  year={2021},
  volume={},
  number={},
  pages={480-487},
  doi={10.1109/ICCD53106.2021.00080}}

@misc{redis,
    title = {{Redis. }},
    howpublished = "\url{https://redis.io/}",
}

@misc{googlecloud,
    title = {{Google Cloud. Memory management best practices}},
    howpublished = "\url{https://cloud.google.com/memorystore/docs/redis/memory-management-best-practices}",
}

@misc{manageredis,
    title = {{Tecton.ai. Managing your Redis Cluster}},
    howpublished = "\url{https://docs.tecton.ai/docs/0.5/setting-up-tecton/setting-up-other-components/managing-your-redis-cluster}",
}

@misc{H100,
    title = {{NVIDIA. NVIDIA H100 Tensor Core GPU}},
    howpublished = "\url{https://www.nvidia.com/en-gb/data-center/h100/}"},
}

@INPROCEEDINGS{amx,
  author={Nassif, Nevine and Munch, Ashley O. and Molnar, Carleton L. and Pasdast, Gerald and Lyer, Sitaraman V. and Yang, Zibing and Mendoza, Oscar and Huddart, Mark and Venkataraman, Srikrishnan and Kandula, Sireesha and Marom, Rafi and Kern, Alexandra M. and Bowhill, Bill and Mulvihill, David R. and Nimmagadda, Srikanth and Kalidindi, Varma and Krause, Jonathan and Haq, Mohammad M. and Sharma, Roopali and Duda, Kevin},
  booktitle={2022 IEEE International Solid- State Circuits Conference (ISSCC)}, 
  title={Sapphire Rapids: The Next-Generation Intel Xeon Scalable Processor}, 
  year={2022},
  volume={65},
  number={},
  pages={44-46},
  doi={10.1109/ISSCC42614.2022.9731107}}

@inproceedings{YCSB,
author = {Cooper, Brian F. and Silberstein, Adam and Tam, Erwin and Ramakrishnan, Raghu and Sears, Russell},
title = {Benchmarking Cloud Serving Systems with YCSB},
year = {2010},
isbn = {9781450300360},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1807128.1807152},
doi = {10.1145/1807128.1807152},
abstract = {While the use of MapReduce systems (such as Hadoop) for large scale data analysis has been widely recognized and studied, we have recently seen an explosion in the number of systems developed for cloud data serving. These newer systems address "cloud OLTP" applications, though they typically do not support ACID transactions. Examples of systems proposed for cloud serving use include BigTable, PNUTS, Cassandra, HBase, Azure, CouchDB, SimpleDB, Voldemort, and many others. Further, they are being applied to a diverse range of applications that differ considerably from traditional (e.g., TPC-C like) serving workloads. The number of emerging cloud serving systems and the wide range of proposed applications, coupled with a lack of apples-to-apples performance comparisons, makes it difficult to understand the tradeoffs between systems and the workloads for which they are suited. We present the "Yahoo! Cloud Serving Benchmark" (YCSB) framework, with the goal of facilitating performance comparisons of the new generation of cloud data serving systems. We define a core set of benchmarks and report results for four widely used systems: Cassandra, HBase, Yahoo!'s PNUTS, and a simple sharded MySQL implementation. We also hope to foster the development of additional cloud benchmark suites that represent other classes of applications by making our benchmark tool available via open source. In this regard, a key feature of the YCSB framework/tool is that it is extensible--it supports easy definition of new workloads, in addition to making it easy to benchmark new systems.},
booktitle = {Proceedings of the 1st ACM Symposium on Cloud Computing},
pages = {143–154},
numpages = {12},
keywords = {benchmarking, cloud serving database},
location = {Indianapolis, Indiana, USA},
series = {SoCC '10}
}

@misc{cpuinference,
    title = {{Timothy P. Morgan. WHY AI INFERENCE WILL REMAIN LARGELY ON THE CPU}},
    howpublished = "\url{https://www.nextplatform.com/2023/04/05/why-ai-inference-will-remain-largely-on-the-cpu/}",
}

@misc{intelbenchmark,
    title = {{Intel Corporation. 4th Generation Intel® Xeon® Scalable Processors Performance.}},
    howpublished = "\url{https://edc.intel.com/content/www/us/en/products/performance/benchmarks/4th-generation-intel-xeon-scalable-processors/}",
}

@misc{vllm,
      title={Efficient Memory Management for Large Language Model Serving with PagedAttention}, 
      author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
      year={2023},
      eprint={2309.06180},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{lightllm,
    title={{Lightllm A Light and Fast Inference Service for LLM}},
    howpublished = "\url{https://github.com/ModelTC/lightllm}",
}

@misc{amdepyc,
    title={{Videocardz. AMD next-gen socket SP7 reportedly launches alongside Venice data-center CPUs}},
    howpublished = "\url{https://videocardz.com/newz/amd-epyc-venice-with-zen6-cpu-cores-reportedly-uses-new-sp7-socket}",
}

@misc{redis-benchmark,
    title={{Redis benchmark. Using the redis-benchmark utility on a Redis server.}},
    howpublished = "\url{https://lore.kernel.org/lkml/CAAPL-u9Wv+nH1VOZTj=9p9S70Y3Qz3+63EkqncRDdHfubsrjfw@mail.gmail.com/}",
}

@inproceedings{fpgaasic,
author = {Kuon, Ian and Rose, Jonathan},
title = {Measuring the Gap between FPGAs and ASICs},
year = {2006},
isbn = {1595932925},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/1117201.1117205},
doi = {10.1145/1117201.1117205},
abstract = {This paper presents experimental measurements of the differences between a 90nm CMOS FPGA and 90nm CMOS Standard Cell ASICs in terms of logic density, circuit speed and power consumption. We are motivated to make these measurements to enable system designers to make better informed hoices between these two media and to give insight to FPGA makers on the deficiencies to attack and thereby improve FPGAs. In the paper, we describe the methodology by which the measurements were obtained and we show that, for circuits containing only combinational logic and flip-flops, the ratio of silicon area required to implement them in FPGAs and ASICs is on average 40. Modern FPGAs also contain "hard" blocks such as multiplier/accumulators and block memories and we find that these blocks reduce this average area gap significantly to as little as 21. The ratio of critical path delay, from FPGA to ASIC, is roughly 3 to 4, with less influence from block memory and hard multipliers. The dynamic power onsumption ratio is approximately 12 times and, with hard blocks, this gap generally becomes smaller.},
booktitle = {Proceedings of the 2006 ACM/SIGDA 14th International Symposium on Field Programmable Gate Arrays},
pages = {21–30},
numpages = {10},
keywords = {FPGA, ASIC, area comparison, power comparison, delay comparison},
location = {Monterey, California, USA},
series = {FPGA '06}
}

@misc{intelsnc,
    title={{Intel Xeon Processor Scalable Family Technical Overview.}},
    howpublished = "\url{https://www.intel.com/content/www/us/en/developer/articles/technical/xeon-processor-scalable-family-technical-overview.html}",
}

@misc{gh200,
title={{Nvidia GH200 Datasheet.}},
howpublished="\url{https://resources.nvidia.com/en-us-dgx-gh200/nvidia-grace-hopper-superchip-datasheet}",
}
@misc{appleuma,
title={{Apple Introduces M2 Ultra.}},
howpublished="\url{https://www.apple.com/newsroom/2023/06/apple-introduces-m2-ultra/}",
}
@misc{pcieopt,
title={{PCI-SIG explores an optical interconnect for higher PCIe performance}},
howpublished="\url{https://www.eenewseurope.com/en/pci-sig-explores-an-optical-connections-for-higher-pcie-performance/}",
}
@misc{uec,
title={{Ultra Ethernet Consortium.}},
howpublished="\url{https://ultraethernet.org/}",
}

@inproceedings{tpuv4,
author = {N. Jouppi and et al.},
title = {TPU v4: An Optically Reconfigurable Supercomputer for Machine Learning with Hardware Support for Embeddings},
year = {2023},
booktitle = {Proceedings of the 50th Annual International Symposium on Computer Architecture},
}

@misc{cxl-centric,
title={{A Case for CXL-Centric Server Processors.}},
author={A. Cho and et al.},
howpublished="\url{https://arxiv.org/abs/2305.05033}",
}

@inproceedings{moe-iclr17,
author = {N. Shazeer and et al.},
title = {Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer},
year = {2017},
booktitle = {The 5th International Conference on Learning Representations},
}

@misc{gpt4,
title={{GPT-4 Architecture, Infrastructure, Training Dataset, Costs, Vision, MoE.}},
author={D. Patel, G. Wong},
year={2023},
howpublished="\url{https://www.semianalysis.com/p/gpt-4-architecture-infrastructure}",
}

@inproceedings{cxldatabase,
author = {Ahn, Minseon and Chang, Andrew and Lee, Donghun and Gim, Jongmin and Kim, Jungmin and Jung, Jaemin and Rebholz, Oliver and Pham, Vincent and Malladi, Krishna and Ki, Yang Seok},
title = {Enabling CXL Memory Expansion for In-Memory Database Management Systems},
year = {2022},
isbn = {9781450393782},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3533737.3535090},
doi = {10.1145/3533737.3535090},
abstract = {Limited memory volume is always a performance bottleneck in an in-memory database management system (IMDBMS) as the data size keeps increasing. To overcome the physical memory limitation, heterogeneous and disaggregated computing platforms are proposed, such as Gen-Z, CCIX, OpenCAPI, and CXL. In this work, we introduce flexible CXL memory expansion using a CXL type 3 prototype and evaluate its performance in an IMDBMS. Our evaluation shows that CXL memory devices interfaced with PCIe Gen5 are appropriate for memory expansion with nearly no throughput degradation in OLTP workloads and less than 8\% throughput degradation in OLAP workloads. Thus, CXL memory is a good candidate for memory expansion with lower TCO in IMDBMSs.},
booktitle = {Proceedings of the 18th International Workshop on Data Management on New Hardware},
articleno = {8},
numpages = {5},
keywords = {CXL, Database Management Systems, Compute Express Link, In-Memory Database, DBMS},
location = {Philadelphia, PA, USA},
series = {DaMoN '22}
}

@INPROCEEDINGS{snoopfilter,
  author={Sharma, Debendra Das},
  booktitle={2022 IEEE Symposium on High-Performance Interconnects (HOTI)}, 
  title={Compute Express Link®: An open industry-standard interconnect enabling heterogeneous data-centric computing}, 
  year={2022},
  volume={},
  number={},
  pages={5-12},
  doi={10.1109/HOTI55740.2022.00017}}

@article{sparkmemory,
author = {Zhang, Xuechen and Khanal, Ujjwal and Zhao, Xinghui and Ficklin, Stephen},
title = {Making Sense of Performance in In-Memory Computing Frameworks for Scientific Data Analysis: A Case Study of the Spark System},
year = {2018},
issue_date = {Oct 2018},
publisher = {Academic Press, Inc.},
address = {USA},
volume = {120},
number = {C},
issn = {0743-7315},
url = {https://doi.org/10.1016/j.jpdc.2017.10.016},
doi = {10.1016/j.jpdc.2017.10.016},
journal = {J. Parallel Distrib. Comput.},
month = {oct},
pages = {369–382},
numpages = {14},
keywords = {SciDB, Spark, In-memory computing, Scientific data analytics}
}

@INPROCEEDINGS{hybridcxleval,
  author={Yang, Qirui and Jin, Runyu and Davis, Bridget and Inupakutika, Devasena and Zhao, Ming},
  booktitle={2022 IEEE International Conference on Networking, Architecture and Storage (NAS)}, 
  title={Performance Evaluation on CXL-enabled Hybrid Memory Pool}, 
  year={2022},
  volume={},
  number={},
  pages={1-5},
  doi={10.1109/NAS55553.2022.9925356}}

@misc{h100amazon,
title={{Nvidia H100 80GB on Amazon.}},
year={2023},
howpublished="\url{https://www.amazon.com/Tesla-NVIDIA-Learning-Compute-Graphics/dp/B0C3XH4QSJ}",
}

@misc{msssd,
title={{Samsung Memory Semantic SSD.}},
year={2023},
howpublished="\url{https://samsungmsl.com/ms-ssd/}",
}

@INPROCEEDINGS{elasticcomputing,
  author={Yi, Sangho and Kondo, Derrick and Andrzejak, Artur},
  booktitle={2010 IEEE 3rd International Conference on Cloud Computing}, 
  title={Reducing Costs of Spot Instances via Checkpointing in the Amazon Elastic Compute Cloud}, 
  year={2010},
  volume={},
  number={},
  pages={236-243},
  doi={10.1109/CLOUD.2010.35}}

@misc{awsm7a,
title={{Amazon EC2 M7a Instances}},
year={2023},
howpublished="\url{https://aws.amazon.com/ec2/instance-types/m7a/}",
}

@misc{awsm7i,
title={{Amazon EC2 M7i Instances}},
year={2023},
howpublished="\url{https://aws.amazon.com/ec2/instance-types/m7i/}",
}

@misc{redisenterprise,
title={{Redis enterprise}},
year={2023},
howpublished="\url{https://redis.io/docs/about/redis-enterprise/}",
}

@misc{numaautobalancing,
title={{NUMA balancing: optimize memory placement for memory tiering system}},
howpublished="\url{https://lore.kernel.org/linux-mm/20220221084529.1052339-1-ying.huang@intel.com/}",
}

@misc{tpppatch,
title={{Transparent Page Placement for Tiered-Memory}},
howpublished = "\url{https://lore.kernel.org/all/cover.1637778851.git.hasanalmaruf@fb.com/}",
}

@misc{redisautotiering,
title={{Auto Tiering Extend Redis Enterprise databases beyond DRAM limits}},
howpublished = "\url{https://redis.com/redis-enterprise/technology/auto-tiering/#:~:text=Redis%20Enterprise's%20auto%20tiering%20lets,compared%20to%20only%20DRAM%20deployments.}",
}


@inproceedings {caerus,
author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Jingrong Chen and Ion Stoica},
title = {Caerus: {NIMBLE} Task Scheduling for Serverless Analytics},
booktitle = {18th USENIX Symposium on Networked Systems Design and Implementation (NSDI 21)},
year = {2021},
isbn = {978-1-939133-21-2},
pages = {653--669},
url = {https://www.usenix.org/conference/nsdi21/presentation/zhang-hong},
publisher = {USENIX Association},
month = apr
}

@inproceedings {shepherd,
author = {Hong Zhang and Yupeng Tang and Anurag Khandelwal and Ion Stoica},
title = {{SHEPHERD}: Serving {DNNs} in the Wild},
booktitle = {20th USENIX Symposium on Networked Systems Design and Implementation (NSDI 23)},
year = {2023},
isbn = {978-1-939133-33-5},
address = {Boston, MA},
pages = {787--808},
url = {https://www.usenix.org/conference/nsdi23/presentation/zhang-hong},
publisher = {USENIX Association},
month = apr
}

@inproceedings{jiffy,
author = {Khandelwal, Anurag and Tang, Yupeng and Agarwal, Rachit and Akella, Aditya and Stoica, Ion},
title = {Jiffy: Elastic Far-Memory for Stateful Serverless Analytics},
year = {2022},
isbn = {9781450391627},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3492321.3527539},
doi = {10.1145/3492321.3527539},
abstract = {Stateful serverless analytics can be enabled using a remote memory system for inter-task communication, and for storing and exchanging intermediate data. However, existing systems allocate memory resources at job granularity---jobs specify their memory demands at the time of the submission; and, the system allocates memory equal to the job's demand for the entirety of its lifetime. This leads to resource underutilization and/or performance degradation when intermediate data sizes vary during job execution.This paper presents Jiffy, an elastic far-memory system for stateful serverless analytics that meets the instantaneous memory demand of a job at seconds timescales. Jiffy efficiently multiplexes memory capacity across concurrently running jobs, reducing the overheads of reads and writes to slower persistent storage, resulting in 1.6 -- 2.5\texttimes{} improvements in job execution time over production workloads. Jiffy implementation currently runs on Amazon EC2, enables a wide variety of distributed programming models including MapReduce, Dryad, StreamScope, and Piccolo, and natively supports a large class of analytics applications on AWS Lambda.},
booktitle = {Proceedings of the Seventeenth European Conference on Computer Systems},
pages = {697–713},
numpages = {17},
keywords = {function-as-a-service, serverless computing, far-memory, data analytics},
location = {Rennes, France},
series = {EuroSys '22}
}

@misc{chase,
title={{CHASE: Accelerating Distributed Pointer-Traversals on Disaggregated Memory}},
year={2023},
howpublished="\url{https://arxiv.org/pdf/2305.02388.pdf}",
}

@misc{dataintensive,
  title = "The Cloud Native Convergence: A New Era of Data-Intensive Applications", 
  author = "Dez Blanchfield", 
  howpublished = {\url{https://elnion.com/2023/06/05/the-cloud-native-convergence-a-new-era-of-data-intensive-applications/}}
}


@article{nvlink,
  title={Evaluating modern gpu interconnect: Pcie, nvlink, nv-sli, nvswitch and gpudirect},
  author={Li, Ang and Song, Shuaiwen Leon and Chen, Jieyang and Li, Jiajia and Liu, Xu and Tallent, Nathan R and Barker, Kevin J},
  journal={IEEE Transactions on Parallel and Distributed Systems},
  volume={31},
  number={1},
  pages={94--110},
  year={2019},
  publisher={IEEE}
}