\chapter{Related and Prior Work}

Recent research has proposed multiple approaches to address the challenges introduced by memory disaggregation. Some efforts focus on optimizing applications for disaggregated memory~\cite{farm, aifm, sherman, existing1}, while others aim to transparently port existing applications, shifting the responsibility of performance mitigation to the service or operating system layer~\cite{mind, legoos, fastswap, infiniswap, runtime1, runtime2}. We group the existing works into three categories: swap-based disaggregation, logical memory disaggregation, and specialized data structures, and explain the benefits and limitations of each approach. Specifically, we highlight how each approach interacts with three key requirements—\textbf{transparency} (the ability to use disaggregated memory without significant application re-implementation), \textbf{good application performance} (ensuring that applications do not suffer from significant latency), and \textbf{resource utilization} (ensuring memory is used efficiently and not wasted)—and how they are hindered by three primary challenges—\textbf{inefficient resource multiplexing}, \textbf{high-latency memory access}, and \textbf{diverse performance characteristics of next-generation interconnects}.

\section{Swap-Based Memory Disaggregation}

One of the earliest approaches to memory disaggregation is swap-based disaggregation, implemented in systems like MIND~\cite{mind}, LegoOS~\cite{legoos}, and FastSwap~\cite{fastswap}. These approaches modify the OS to manage disaggregated memory similarly to local memory, transferring 4KB pages between compute and memory nodes. This effectively hides hardware complexity from applications, achieving transparency and addressing the challenge of inefficient resource multiplexing by allowing memory to be shared dynamically across nodes without requiring application re-implementation. Additionally, by pooling memory and allowing dynamic allocation, swap-based systems ensure efficient resource utilization, minimizing waste.

However, swap-based approaches struggle with the high-latency memory access introduced by interconnects like Ethernet. Remote memory access can take 4-5 microseconds, while local memory access typically takes less than 100 nanoseconds~\cite{cxl1, mind}. This significant increase in latency makes it difficult to maintain good application performance, especially for latency-sensitive workloads, which experience noticeable slowdowns in disaggregated environments.

Thus, while swap-based disaggregation achieves transparency and resource utilization by addressing inefficient resource multiplexing, it fails to ensure good application performance due to high-latency memory access in remote environments.

\section{Logical Memory Disaggregation}

Logical memory disaggregation, used in systems like VMware’s memory pool~\cite{pool1}, AIFM~\cite{aifm}, and Pocket~\cite{pocket}, manages memory resources logically within the existing server architecture without requiring hardware changes. By placing computation close to memory, logical disaggregation minimizes latency, overcoming the challenge of high-latency memory access and improving application performance for certain workloads.

However, this approach struggles with the challenge of inefficient resource multiplexing, as it retains the tight coupling between memory and compute nodes. This limitation prevents full decoupling of resources at the hardware level, leading to under-utilization and memory fragmentation, which negatively affects resource utilization. Additionally, while it offers some performance improvements, it does not fully achieve transparency because applications must still handle complexities related to memory locality, making it difficult to fully abstract disaggregated memory from the application layer.

Therefore, logical memory disaggregation achieves good application performance but falls short of providing transparency and efficient resource utilization due to tightly coupled resources and limited flexibility in memory management.

\section{Specialized Data Structure Approaches}

A third approach involves directly modifying applications to take advantage of disaggregated hardware, as seen in systems like Sherman~\cite{sherman}, FUSEE~\cite{fusee}, and ROLEX~\cite{rolex}. These approaches design specialized data structures—such as B-trees—optimized for disaggregated environments, where frequently accessed portions are placed near the compute node. This approach significantly reduces access latency and improves application performance, particularly for latency-sensitive applications. Furthermore, by optimizing data structure access patterns, these systems make more efficient use of memory, achieving better resource utilization.

However, specialized data structures fail to achieve transparency, as they require extensive modifications to application code, increasing development overhead. More critically, this approach provides only point solutions for specific types of data structures, such as B-trees or hash maps, and cannot be applied system-wide like swap-based or logical disaggregation. This limitation reduces the broad applicability of the approach and makes it less practical for diverse workloads.

Consequently, while specialized data structures improve application performance and resource utilization, they fail to achieve transparency due to the need for significant application modifications and are limited to specific data structures rather than system-wide applicability.

\section{Summary}

Each of these approaches—swap-based, logical memory disaggregation, and specialized data structures—addresses some of the challenges of memory disaggregation but fails to meet all three key requirements due to specific limitations. Swap-based methods achieve transparency and resource utilization by addressing inefficient resource multiplexing, but suffer from poor performance due to high-latency memory access. Logical memory disaggregation improves application performance by overcoming the challenge of high-latency memory access, but cannot fully decouple resources, limiting its impact on resource utilization and transparency due to inefficient resource multiplexing. Specialized data structures excel in application performance and resource utilization, but fail to provide transparency because of the need for significant application re-implementation, and they only provide point solutions for specific data types.

Additionally, all three approaches are designed for Ethernet-based disaggregation and fail to address the diverse performance characteristics of next-generation interconnects like CXL. As a result, their benefits may not fully extend to future memory disaggregation technologies, highlighting the need for more comprehensive solutions that can adapt to evolving interconnect architectures.




\begin{comment}

Pocket~\cite{pocket} demonstrates how existing designs for in-memory key-value stores~\cite{redis, farm, memcached, memc3, mica, ramcloud, anna, succinct, blowfish}, distributed~\cite{dsm1, dsm2, dsm3, treadmarks}, and disaggregated memory systems~\cite{infiniswap,remoteregions,legoos,mind}, as well as storage systems with flexible interfaces~\cite{udf1, udf2, udf3, storedprocedure1, storedprocedure2, storedprocedure3, boxwood, sinfonia}, can be adapted to achieve three primary goals for intermediate data storage in serverless analytics: low-latency/high-throughput, resource sharing, and elasticity.

Other recent systems have also explored fine-grained resource sharing. Pisces~\cite{pisces} provides per-tenant performance isolation in multi-tenant cloud storage systems but does not share storage capacity across tenants. Memshare~\cite{memshare} facilitates memory sharing across tenants in a KV cache setting, though it evicts KV pairs with lower contribution to hit-rate under high contention.


Prior research has extensively explored processing units in near-memory and processing-in-memory (PIM) architectures~\cite{ahn2015scalable, asghari2016chameleon, dai2018graphh, schuiki2018scalable, mutlu2019processing, lockerman2020livia, tu2022redcim, devic2022_PIM, wang2022_Nearstream, xie2023mpu, mutlu2022modern, oliveira2022accelerating, eckert2022eidetic, chi2016prime, seshadri2017simple, kwon2019_TensorDIMM, boroumand2019_codna, cho2020_data, ke2020_RecNMP, wang2021stream, xie2021spacea, ke2021near, singh2021fpga, olgun2022pidram, dai2022dimmining, gu2020ipim, gomez2023evaluating, walkers, impica}, as well as CPUs~\cite{storagefunctions, splinter, aifm, kayak_nsdi_21, storm_systor_19, zhang2022_teleport} and FPGAs~\cite{clio, strom} near remote/disaggregated memory.

Recent proposals have investigated specialized data structures for disaggregated memory~\cite{sherman, clover, fusee, rolex, marlin, sephash, ditto}, while others have focused on enabling computation offloading to CPUs on memory nodes~\cite{aifm, kayak_nsdi_21, splinter, storagefunctions, storm_systor_19}. FPGA-based approaches have been explored for on-path data processing~\cite{clio, strom}, as well as ASIC-based accelerators for performance and energy efficiency~\cite{impica, walkers}. Additionally, wimpy processors and SmartNICs have been used to offload computation~\cite{rmc_hotnets20, redn}.

In-memory processing has been widely studied across a variety of architectures, including both near-memory and disaggregated architectures~\cite{ahn2015scalable, impica, asghari2016chameleon, chi2016prime, seshadri2017simple, dai2018graphh, schuiki2018scalable, mutlu2019processing, kwon2019_TensorDIMM, boroumand2019_codna, gu2020ipim, lockerman2020livia, cho2020_data, ke2020_RecNMP, wang2021stream, xie2021spacea, ke2021near, singh2021fpga, olgun2022pidram, mutlu2022modern, oliveira2022accelerating, eckert2022eidetic, tu2022redcim, dai2022dimmining, devic2022_PIM, wang2022_Nearstream, gomez2023evaluating, xie2023mpu}. Specialized partitioning and allocation policies for pointer traversals in disaggregated memory have also been explored~\cite{sherman, clover, fusee, rolex, marlin, sephash, ditto}.

Finally, related work has examined distributed execution and pointer traversals on network-attached memory, though these approaches do not fully address all the performance, scalability, and energy efficiency challenges in disaggregated architectures~\cite{storagefunctions, splinter, aifm, kayak_nsdi_21, walkers, clio, strom, sun2023demystifying}.


As memory-intensive applications like machine learning and High-Performance Computing (HPC) continue to grow, expanding memory capacity and bandwidth has become a critical challenge~\cite{dataintensive, FlatFlash, cxl-ssd}. Compute Express Link (CXL)~\cite{cxl, cxl_azure, cxlcentric} has emerged as a promising solution, offering a novel interconnect technology that connects external memory devices via PCIe, significantly enhancing both memory capacity and bandwidth.

Research on CXL technology is extensive. Studies such as \cite{cxl_azure, cxlcentric, demystify} explore CXL’s potential for resource disaggregation and memory expansion, highlighting its ability to address memory bottlenecks in large-scale applications. However, much of the research relies on simulations~\cite{pond, cxlcentric} or FPGA-based CXL hardware~\cite{demystify, intelfpga, directcxl}, limiting practical insights into production-ready ASIC-based hardware. More recent empirical evaluations, such as \cite{demystify, smt}, have begun to investigate the performance of ASIC-based CXL hardware, offering valuable data on its real-world potential. These studies show that while CXL introduces higher latency compared to local memory, this gap narrows for cross-socket memory access, making CXL a feasible option for memory tiering.

Other work, such as \cite{pond, tpp, directcxl}, has examined the trade-offs between performance and cost in CXL systems, particularly through synthetic benchmarks. For instance, \cite{tpp} investigates kernel-level techniques for promoting hot pages from slower to faster memory tiers to boost performance while maintaining application transparency. Similarly, \cite{CXLPoolCost} introduces cost models for memory pooling in CXL, but further research is needed to evaluate the economic viability of migrating specific workloads to CXL-enabled systems.



\section{Summary}

\end{comment}